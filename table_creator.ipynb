{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "table_creator",
      "provenance": [],
      "collapsed_sections": [
        "ewqrMebK3ebd"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewqrMebK3ebd"
      },
      "source": [
        "# The Ripple Effect-Table Creator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPXYx-Tuhbwq"
      },
      "source": [
        "# Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYgH_jq85TZK"
      },
      "source": [
        "## Inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10Vfy_b-5Url"
      },
      "source": [
        "Impact Statements\n",
        "- Great Lakes Foodweb\n",
        "- NOAA's Current Impact Statements\n",
        "- NOAA's Reference List\n",
        "- Nonindigenous aquatic species (what is invasive)\n",
        "- Waterlife Data\n",
        "- Technical Memorandum\n",
        "- Species ID List\n",
        "\n",
        "Networks\n",
        "- [NHDPlus Great Lakes Data (Vector Processing Unit 04) | US EPA](https://www.epa.gov/waterdata/nhdplus-great-lakes-data-vector-processing-unit-04)\n",
        "- USGS geojson data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrVoY-ta5R-f"
      },
      "source": [
        "## Outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Af1afNh36k0"
      },
      "source": [
        "Impact Statements For NOAA\n",
        "- impact_statements.xlsx\n",
        "\n",
        "\n",
        "Tables for our Database\n",
        "- Network Graph\n",
        "  - impact_rel.csv\n",
        "  - invasive_species.csv\n",
        "  - species.csv\n",
        "- Waterways Map\n",
        "  - species_observed.csv\n",
        "- Ripple Plot\n",
        "  - target_dropdown_rel.csv\n",
        "  - target_dropdown_impacted.csv\n",
        "  - target_dopdown_impacter.csv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRClvTYy7Z7Q"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABrmFAd57fli",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c33dcdd-e1c2-43c1-b80e-e39121529064"
      },
      "source": [
        "!apt-get install build-essential libpoppler-cpp-dev pkg-config python-dev\n",
        "!pip install pdftotext\n",
        "\n",
        "import pdftotext\n",
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import urllib.request\n",
        "import networkx as nx\n",
        "import datetime as dt\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import Normalizer, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from scipy.spatial import cKDTree\n",
        "from sklearn.neighbors import BallTree, KDTree\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('assets')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.4ubuntu1).\n",
            "pkg-config is already the newest version (0.29.1-0ubuntu2).\n",
            "python-dev is already the newest version (2.7.15~rc1-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libpoppler-cpp0v5\n",
            "The following NEW packages will be installed:\n",
            "  libpoppler-cpp-dev libpoppler-cpp0v5\n",
            "0 upgraded, 2 newly installed, 0 to remove and 40 not upgraded.\n",
            "Need to get 36.7 kB of archives.\n",
            "After this operation, 188 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpoppler-cpp0v5 amd64 0.62.0-2ubuntu2.12 [28.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpoppler-cpp-dev amd64 0.62.0-2ubuntu2.12 [8,676 B]\n",
            "Fetched 36.7 kB in 0s (85.5 kB/s)\n",
            "Selecting previously unselected package libpoppler-cpp0v5:amd64.\n",
            "(Reading database ... 148486 files and directories currently installed.)\n",
            "Preparing to unpack .../libpoppler-cpp0v5_0.62.0-2ubuntu2.12_amd64.deb ...\n",
            "Unpacking libpoppler-cpp0v5:amd64 (0.62.0-2ubuntu2.12) ...\n",
            "Selecting previously unselected package libpoppler-cpp-dev:amd64.\n",
            "Preparing to unpack .../libpoppler-cpp-dev_0.62.0-2ubuntu2.12_amd64.deb ...\n",
            "Unpacking libpoppler-cpp-dev:amd64 (0.62.0-2ubuntu2.12) ...\n",
            "Setting up libpoppler-cpp0v5:amd64 (0.62.0-2ubuntu2.12) ...\n",
            "Setting up libpoppler-cpp-dev:amd64 (0.62.0-2ubuntu2.12) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Collecting pdftotext\n",
            "  Downloading pdftotext-2.2.0.tar.gz (113 kB)\n",
            "\u001b[K     |████████████████████████████████| 113 kB 32.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pdftotext\n",
            "  Building wheel for pdftotext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pdftotext: filename=pdftotext-2.2.0-cp37-cp37m-linux_x86_64.whl size=54957 sha256=a9661822eda9370c35f2b18fe86b5963303a23e226c6a254820a9c049080d923\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/3f/b1/bfc92d0baea1ec857b379453792753e2639070eb49b6f9c0d3\n",
            "Successfully built pdftotext\n",
            "Installing collected packages: pdftotext\n",
            "Successfully installed pdftotext-2.2.0\n",
            "Mounted at assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLsNvK0N9g-P"
      },
      "source": [
        "## File Paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5XMw1tE7jbE"
      },
      "source": [
        "main_path = '/content/assets/Shared drives/ermiasb-rjbowman-tobyk/'\n",
        "data_path = main_path + 'data/'\n",
        "assets_path = main_path + 'assets/'\n",
        "results_path = main_path + 'results/'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpDRVBOL9dkG"
      },
      "source": [
        "## Import Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUnYpd1E9fD7"
      },
      "source": [
        "# Impact Statement Inputs\n",
        "foodweb_file = data_path + \"FoodWeb-Fact-Sheet-Matrices.xlsx\"\n",
        "waterlife_data_excel_file = data_path + 'Waterlife_data_5272021.xlsx'\n",
        "old_impact_statements_file = data_path + \"2021-07-16-impacts.csv\"\n",
        "noaa_invasive_species_file = data_path + 'glansis-species.csv'\n",
        "noaa_watchlist_species_file = data_path + 'glansis-watchlist-species.csv'\n",
        "impact_references_file = data_path + 'impact_statement_references_existing.csv'\n",
        "tsn_file = data_path + 'species_to_tsn.csv'\n",
        "species_to_species_id_file = data_path + 'species_to_species_id.csv'\n",
        "reference_match_file = data_path +'references_match.xlsx'\n",
        "existing_reference_path = data_path + 'existing_references.xlsx'\n",
        "species_id_to_scientific_file = data_path + 'species_id_to_scientific.csv'\n",
        "\n",
        "# Network Inputs\n",
        "rivers_file = data_path + 'rivers.geojson'\n",
        "lakes_file = data_path + 'lake.geojson'\n",
        "invasion_file = data_path + 'NAS-Data-Download-06-09-2021-59463.csv'\n",
        "waterways_edges_file = data_path + \"waterway_edges.csv\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZNWeuzeGIab"
      },
      "source": [
        "## Asset Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9eNZbTYIK4w"
      },
      "source": [
        "# Impact Statement Asset Outputs\n",
        "impacted_impacter_file = assets_path + 'impacter_impacted.csv'\n",
        "index_to_species_file = assets_path + 'index_to_species.csv'\n",
        "index_to_species_file = assets_path + 'index_to_species.csv'\n",
        "impact_line_file = assets_path + 'impact_lines.txt'\n",
        "impact_statement_file = assets_path + 'impact_statements.txt'\n",
        "pred_prey_file = assets_path + 'pred_prey.csv'\n",
        "impact_relationships_file = assets_path + 'impact_relationships.txt'\n",
        "references_file = assets_path + '/references.txt'\n",
        "database_upload_file = assets_path + '/database_upload.txt'\n",
        "scientific_to_common_file = assets_path + 'scientific_to_common.csv'\n",
        "impacter_impacted_distance_named_file = assets_path + 'impacter_impacted_distance_named.csv'\n",
        "\n",
        "# Network Asset Outputs\n",
        "observations_waterways_distances_file = assets_path + 'ovservations_waterways_distance.csv'\n",
        "species_to_index_file = assets_path + \"species_to_index.csv\"\n",
        "index_to_species_file = assets_path + \"index_to_species.csv\"\n",
        "relationships_named_file = assets_path +'relationships_file_named.csv'\n",
        "relationships_keyed_file = assets_path + 'relationships_file_keyed.csv'\n",
        "waterways_dataframe_file = assets_path + 'waterways.csv'\n",
        "waterways_file = assets_path + 'waterways.p'\n",
        "specimens_locations_file = assets_path + \"specimen_locations.csv\"\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_7mOpTc9aHj"
      },
      "source": [
        "## Export Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FgopWaC9WZu"
      },
      "source": [
        "# Impact Statement Outputs for NOAA\n",
        "impact_statements_file = results_path + 'impact_statements.xlsx'\n",
        "\n",
        "# Table Outputs for Database\n",
        "\n",
        "#Network Graph\n",
        "species_file = results_path + 'species.csv'\n",
        "impact_rel_file =  results_path + 'impact_rel.csv'\n",
        "invasive_species_file = results_path + 'invasive_species.csv'\n",
        "\n",
        "# Waterways Map\n",
        "species_observed_file = results_path + 'species_observed.csv'\n",
        "\n",
        "# Ripple Outputs\n",
        "invasive_impacters_dropdown_file = results_path + 'target_dropdown_impacter.csv'\n",
        "impacted_species_dropdown_file = results_path + 'target_dropdown_impacted.csv'\n",
        "impacter_impacted_distance_file = results_path + 'target_data_rel.csv'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5I4lHB5kEVtX"
      },
      "source": [
        "## Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSnCCxhbBwrI"
      },
      "source": [
        "NOAA_url = 'https://www.glerl.noaa.gov/pubs/tech_reports/glerl-'\n",
        "technical_files = ['tm-161.pdf', 'tm-161b.pdf', 'tm-161c.pdf', 'tm-169.pdf', \n",
        "                   'tm-169b.pdf', 'tm-169c.pdf']\n",
        "\n",
        "sheet_names = ['Lake Michigan', 'Lake Huron', 'Lake Superior', \n",
        "               'Lake Ontario', 'Lake Erie', 'Lake St. Clair']"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaVXMioW7EyX"
      },
      "source": [
        "# Creating Impact Statements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ww-itZAKink"
      },
      "source": [
        "## Creating Tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeV2JEY5KlD4"
      },
      "source": [
        "def dictionary_to_csv(dictionary, filepath):\n",
        "  \"\"\"\n",
        "  inputs a dictionary and a filepath\n",
        "  outputs (saves) a csv file \n",
        "  \"\"\"\n",
        "  with open(filepath, 'w') as file:\n",
        "    writer = csv.writer(file)\n",
        "    for key, value in dictionary.items():\n",
        "      writer.writerow([key, value])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyMmE2KSARyG"
      },
      "source": [
        "## Downloading Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLHZcfJSARN9"
      },
      "source": [
        "def download_NOAA_technical_memorandums(base_url, download_list):\n",
        "  \"\"\"\n",
        "  inputs a base URL for NOAA's technical memorandums and a list of files to download\n",
        "  downloads the files and saves them to the asset path\n",
        "  returns None\n",
        "  \"\"\"\n",
        "  \n",
        "  print('Downloading NOAA technical memorandum files....')\n",
        "\n",
        "  existing_files = os.listdir(assets_path)\n",
        "  \n",
        "  for file_name in download_list:\n",
        "    if file_name not in existing_files:\n",
        "      url_portion = file_name.split('-')[-1]\n",
        "      url_portion = url_portion.split('.')[0]\n",
        "      urllib.request.urlretrieve(base_url + url_portion + '/' + file_name, assets_path + file_name)\n",
        "  \n",
        "  return None"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWuem4NiEP0v"
      },
      "source": [
        "## Cleaning Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-gWOAeMEipx"
      },
      "source": [
        "def fix_document_errors(pdf_file, page_idx, lines):\n",
        "  \"\"\"\n",
        "  inputs a pdf file, a page from that file, and all the lines from that page\n",
        "  changes data to fix errors in the orignal document\n",
        "  returns corrected lines\n",
        "  \"\"\"\n",
        "\n",
        "  if pdf_file == 'tm-161.pdf':\n",
        "    if page_idx == 69:\n",
        "      lines[19] = ''\n",
        "      lines[20] = '• Anecdotal observations in the early 20th century spurred the reputation of G. affinis as a successful control' # fixes parsing for 20th resulting on 2 lines\n",
        "    elif page_idx == 437:\n",
        "      lines[2] = 'Common Name: Redtop' # Fixes a type 'Retop' in document\n",
        "    elif page_idx == 727:\n",
        "      lines.insert(26, 'Does the species have some medicinal or research value (outside of research geared towards its control)?') # fixes missing line in document\n",
        "    elif page_idx == 939:\n",
        "      lines[0] = 'Scientific Name: Viral hemorrhagic septocemia Virus (Family Novirhabdoviridae, Order Mononegavirales) Genotype IV sublineage b' # fixes scientific name on multiple lines\n",
        "    elif page_idx == 1018:\n",
        "      lines[25] = 'Van Overdijk, C.D.A., I.A. Grigorovich, T. Mabee, W.J. Ray, J.J.H. Ciborowski, and H.J. MacIsaac.' # capitalizes van\n",
        "  \n",
        "  elif pdf_file == 'tm-161b.pdf':\n",
        "    if page_idx == 6:\n",
        "      lines[27] = '●   Could potentially compete with other cladocerans for algal food sources, but this has not been documented' # Fixes missing \\n in document\n",
        "\n",
        "  elif pdf_file == 'tm-169.pdf':\n",
        "    if page_idx == 328:\n",
        "      lines[2] = 'Common Name: Blue Catfish, White Cat, White Fulton, Fulton, Humpback Blue, Forktail Cat, Blue Channel Catfish' # fixes common names on multiple lines\n",
        "    elif page_idx == 1201:\n",
        "      lines[3] = 'Common Name: Harris Mud Crab, Estuarine Mud Crab, Dwarf Mud Crab, White-fingered (or white-tipped) Mud Crab' # fixes common names on multiple lines\n",
        "    elif page_idx == 1537:\n",
        "      lines[31] = 'Bij de Vaate, A., K. Jażdżewski, H. A. M. Ketelaars, S. Gollasch, and G. van der Velde. Geographical' # capitalizes bij\n",
        "    elif page_idx == 1552:\n",
        "      lines[0] = 'De Kluijver, M.J., and S.S. Ingalsuo. Macrobenthos of the North Sea- Crustacea. Corophium curvispinum' # capitalize D\n",
        "      lines[3] = 'De la Cruz, A. Va CNA contra la plaga del ‘repollito’. Conexion Total August 12, 2014 (2014).' # capitalize D\n",
        "      lines[20] = 'Den Hartog, C., and G. Van der Velde. Invasions by plants and animals into coastal, brackish, and fresh' # capitalize D\n",
        "      lines[22] = 'Den Hartog, C., F. van den Brink, and G. van der Velde. Why was the invasion of the river Rhine by' # capitalize D\n",
        "    elif page_idx == 1619:\n",
        "      lines[3] = 'Scipiloti, D. Fish community in the Stagnone di Marsala: Distribution and resource partitioning as a' # move initial after last name\n",
        "    elif page_idx == 1630:\n",
        "      lines[29] = 'Van den Brink F.W.B, G. van der Velde, and A. bij de Vaate. 1991. Amphipod invasion on the Rhine.' # capitalizes van\n",
        "      lines[31] = 'Van den Brink, F.W.B., G. van der Velde, and A. bij de Vaate. Ecological aspects, explosive range' # capitalizes van\n",
        "      lines[34] = 'Van Densen, W.L.T. Piscivory and the development of bimodality in the size distribution of 0+ pikeperch' # capitalizes van\n",
        "    elif page_idx == 1631:\n",
        "      lines[7] = 'Van der Velde, G., S. Rajagopal, B. Kelleher, I. Musko, B. Vaate, and F. Schram. Ecological impact of' # capitalize van\n",
        "      lines[10] = 'Van der Velde, G., R.S.E.W. Leuven, D. Platvoet, K. Bacela, M.A.J. Huijbregts, H.W.M. Hendriks, and' # capitalize van\n",
        "      lines[14] = 'Van Dijk, G.M., and B. van Zanten. Seasonal changes in zooplankton abundance in the lower Rhine' # capitalize van\n",
        "      lines[18] = 'Van Haaren, T., and J. Soors. Sinelobus stanfordi (Richardson, 1901): A new crustacean invader in' # capitalize van\n",
        "      lines[20] = 'Van Kessel, N., M. Dorenbosch, M.R.M. De Boer, R.S.E.W. Leuven, and G. Van der Velde. Competition' # capitalize van\n",
        "      lines[23] = 'Van Overdijk, C.D.A., I.A. Grigorovich, T. Mabee, W.J. Ray, J.J.H. Ciborowski, and H.I. MacIsaac.' # capitalize van\n",
        "      lines[26] = 'Van Riel, M.C., G. van der Velde, and A. bij de Vaate. To conquer and persist: colonization and' # capitalize van\n",
        "\n",
        "  elif pdf_file == 'tm-169b.pdf':\n",
        "    if page_idx == 69:\n",
        "      lines.insert(4, 'Unknown') # fixes missing line in document\n",
        "    elif page_idx == 52:\n",
        "      lines.insert(-3, 'Does it diminish the perceived aesthetic or natural value of the areas it inhabits') # fixes missing questions\n",
        "    elif page_idx == 222:      \n",
        "      lines.insert(23, 'Does it diminish the perceived aesthetic or natural value of the areas it inhabits') # fixes missing questions\n",
        "      lines.insert(23, 'Does it inhibit recreational activities and/or associated tourism') # fixes missing questions\n",
        "      lines.insert(21, 'Does it negatively affect water quality') # fixes missing questions\n",
        "      lines.insert(21, 'Does it cause damage to infrastructure') # fixes missing questions\n",
        "      lines.insert(21, 'Does the species pose some hazard or threat to human health') # fixes missing questions\n",
        "    elif page_idx == 247:\n",
        "      lines[6] = 'Bij de Vaate, A. 2003. Degradation and recovery of the freshwater fauna in the lower sections of the' # capitalize bij\n",
        "      lines[8] = 'Bij de Vaate, A., K. Jazdzewski, H. A. M. Ketelaars, S. Gollasch, and G. van der Velde. 2002.' # capitalize bij\n",
        "    elif page_idx == 249:\n",
        "      lines[32] = 'De Kluijver, M.J., and S.S. Ingalsuo. 1999. Macrobenthos of the North Sea- Crustacea. Corophium' # capitalize de\n",
        "    elif page_idx == 250:\n",
        "      lines[0] = 'Den Hartog, C., F. van den Brink, and G. van der Velde. 1992. Why was the invasion of the river' # capitalize den\n",
        "    elif page_idx == 260:\n",
        "      lines[1] = 'molitrix) planktivory in a floodplain lake of the lower Mississippi River basin. J Fresh Ecol 25:85–93.' # adds period to end reference\n",
        "    elif page_idx == 263:\n",
        "      lines[37] = 'Van den Brink, F., G. van der Velde, and A. bij de Vaate. 1989. A note on the immigration of' # capitalize van\n",
        "    elif page_idx == 264:\n",
        "      lines[0] = 'Van den Brink, F.W.B., G. van der Velde, and A. bij de Vaate. 1993. Ecological aspects, explosive' # capitalize van\n",
        "      lines[3] = 'Van der Velde, G., B.G.P. Paffen, and F.W.B van den Brink. 1994. Decline of zebra mussel' # capitalize van\n",
        "      lines[6] = 'Van der Velde, G., S. Rajagopal, B. Kelleher, I. Musko, B. Vaate, and F. Schram. 2000. Ecological' # capitalize van\n",
        "      lines[9] = 'Van der Velde, G., S. Rajagopal, F. van den Brink, B. Kelleher, B. Paffen, A. Kempers, and A. bij de' # capitalize van\n",
        "      lines[13] = 'Van Riel, M.C., G. van der Velde, and A. bij de Vaate. 2006. To conquer and persist: colonization and' # capitalize van\n",
        "\n",
        "  elif pdf_file == 'tm-169c.pdf':\n",
        "    if page_idx == 21:\n",
        "      lines.insert(11, 'Does it outcompete native species for available resources') # fixes missing questions\n",
        "    elif page_idx == 94:\n",
        "      lines.pop(27) # fixes checkbox occuring on wrong line\n",
        "    elif page_idx == 147:\n",
        "      lines.pop(24) # fixes checkbox occuring on wrong line\n",
        "    elif page_idx == 151:\n",
        "      lines.pop(17) # fixes checkbox occuring on wrong line\n",
        "    elif page_idx == 173:\n",
        "      lines[22] = 'signal crayfish and juvenile Atlantic salmon. Journal of Fish Biology. 65(2):437-44.' # adds period to end reference\n",
        "      lines[34] = 'cause massive population decline of an invasive crayfish. Freshwater Biology. 52(6): 1134-1146.' # adds period to end reference\n",
        "    elif page_idx == 176:\n",
        "      lines[31] = 'Ludwigia (Onagraceae) on the middle Loire River, France. Aquatic Botany. 90: 143-148.' # adds period to end reference\n",
        "    elif page_idx == 177:\n",
        "      lines[20] = 'affects pollinator visitants to a native plant at high abundances. Aquatic Invasions. 3: 357-367.' # adds period to end reference\n",
        "      lines[22] = 'plants and macroinvertebrates in temperate ponds. Biological Invasions. 13: 2715-2726.' # adds period to end reference, adds space between plants and \n",
        "\n",
        "  return lines"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TFgdGDQEisq"
      },
      "source": [
        "def clean_page(page):\n",
        "  \"\"\"\n",
        "  inputs a pdf page as string\n",
        "  removes formating characters\n",
        "  returns a string\n",
        "  \"\"\"\n",
        "\n",
        "  page = page.replace('\\t', '')\n",
        "  page = page.replace('\\r', '')\n",
        "  page = page.replace('\\xa0', '')          \n",
        "\n",
        "  return page"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdadT9UBEivM"
      },
      "source": [
        "def clean_lines(page):\n",
        "  \"\"\"\n",
        "  inputs a pdf page as string\n",
        "  breaks lines of the pdf\n",
        "    removes empty lines, which is necessary for page number detection\n",
        "  returns a list of lines\n",
        "  \"\"\"\n",
        "  clean_lines = []\n",
        "  lines = page.split('\\n')\n",
        "  for line in lines:\n",
        "    line = line.strip()\n",
        "    if line:\n",
        "      clean_lines.append(line)\n",
        "  return clean_lines"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md1NlGkxE1yj"
      },
      "source": [
        "## Detecting Document Sections"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYMOlLq0Eixp"
      },
      "source": [
        "def detect_page_number(line, page_number):\n",
        "  \"\"\"\n",
        "  inputs a line from page and the current page number\n",
        "  detects if the line is an integer and is the next expected page number\n",
        "  returns whether the page number\n",
        "  \"\"\"\n",
        "  \n",
        "  if line.isnumeric():\n",
        "    if int(line) == page_number + 1:\n",
        "      page_number = int(line)\n",
        "  return page_number"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v001NG3cEi0W"
      },
      "source": [
        "def detect_taxonimic_group(line, taxonomic_group):\n",
        "  \"\"\"\n",
        "  inputs a line and the current taxonomic_group\n",
        "  detects if the line contains a new taxonomic group\n",
        "  returns the taxonomic group\n",
        "  \"\"\"\n",
        "  \n",
        "  if line[:2] == 'A.' and line[2].isnumeric():\n",
        "    if not line[-1].isnumeric():\n",
        "      line_list = line.split()\n",
        "      taxonomic_group = ' '.join(line_list[1:]).strip()\n",
        "\n",
        "  return taxonomic_group"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ov-heviMEi20"
      },
      "source": [
        "def detect_scientific_name(line, scientific_name, page_number):\n",
        "  \"\"\"\n",
        "  inputs line of text as a string and the current scientific name and the page_number\n",
        "  detects if there is a new scientific name\n",
        "  returns if we scientific name\n",
        "  \"\"\"\n",
        "\n",
        "  if 'Scientific Name:' in line and page_number > 0:\n",
        "    scientific_name = line.split('Scientific Name:')[-1].strip()\n",
        "  return scientific_name"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vo7a7wfuEi5W"
      },
      "source": [
        "def detect_common_name(line, common_name):  \n",
        "  \"\"\"\n",
        "  inputs a line of text as a string and the current common name\n",
        "  detects if a new common name is listed\n",
        "  returns the common name as a list\n",
        "  \"\"\"\n",
        "\n",
        "  clean_common_names = common_name\n",
        "  if 'Common Name' in line:\n",
        "    line = line.replace('Common Name(s):','Common Name:')\n",
        "    line = line.replace('none','')\n",
        "    line = line.replace('None','')\n",
        "    clean_common_names = []\n",
        "    common_names = line.split('Common Name:')[-1].strip()\n",
        "    common_names = common_names.split(',')\n",
        "    for common_name in common_names:\n",
        "      common_name = common_name.strip().lower()\n",
        "      if common_name:\n",
        "        clean_common_names.append(common_name)\n",
        "  return clean_common_names"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjPahFRUEi8E"
      },
      "source": [
        "def detect_section(line, section, section_triggers, section_count, scientific_name, impact_trigger):\n",
        "  \"\"\"\n",
        "  inputs line of text, current section, list of section triggers, \n",
        "    current scientific name, the section count and the impact trigger\n",
        "  detects if we are in a new section\n",
        "    calls the missing section function\n",
        "    changes the section\n",
        "    updates the section count\n",
        "  returns the current section and impact trigger\n",
        "  \"\"\"\n",
        "  \n",
        "  for section_idx, trigger_statements in enumerate(section_triggers):\n",
        "    for trigger_statement in trigger_statements:\n",
        "      if trigger_statement in line and scientific_name:\n",
        "        detect_missing_sections(section, section_idx, scientific_name)\n",
        "        section = section_idx\n",
        "        section_count[section] += 1\n",
        "        impact_trigger = False\n",
        "  return section, impact_trigger"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Znr1bfCFa-n"
      },
      "source": [
        "def detect_missing_sections(current_section, new_section, scientific_name):\n",
        "  \"\"\"\n",
        "  inputs current section, new section index, and scientific_name\n",
        "  prints a warning if sections are missing \n",
        "  returns None\n",
        "  \"\"\"\n",
        "\n",
        "  if (new_section == 0 and (current_section == 1 or current_section == 2)) or \\\n",
        "  (new_section == 2 and current_section != 1) or \\\n",
        "  (new_section == 3 and current_section != 2):\n",
        "    print('Warning: Missing Section for', scientific_name)\n",
        "\n",
        "  return None"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHJvmnDnFcyh"
      },
      "source": [
        "def detect_question(line, section, question, question_triggers, question_count, impact_trigger):\n",
        "  \"\"\"\n",
        "  receives line, current section, current question, list of question_triggers\n",
        "    question_count, and impact_trigger\n",
        "  detects if there is a new question\n",
        "  returns the current quetion and the impact_trigger\n",
        "  \"\"\"\n",
        "\n",
        "  section_question_triggers = question_triggers[section]\n",
        "  for question_idx, section_questions in enumerate(section_question_triggers):\n",
        "    for section_question in section_questions:      \n",
        "      if section_question in line:\n",
        "        question = question_idx\n",
        "        impact_trigger = False\n",
        "        question_count[section][question_idx] += 1\n",
        "  return question, impact_trigger"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8e_aMqVG6QR"
      },
      "source": [
        "def get_section_triggers():\n",
        "  \"\"\"\n",
        "  no inputs\n",
        "  returns a list of strings that denote new sections in the document\n",
        "  \"\"\"\n",
        "\n",
        "  section_triggers = [\n",
        "    ['Scientific Name:',],\n",
        "    ['ENVIRONMENTAL IMPACT',],\n",
        "    ['SOCIO-ECONOMIC IMPACT',],\n",
        "    ['BENEFICIAL EFFECT', 'BENEFICIAL IMPACT',]                                  \n",
        "  ]\n",
        "\n",
        "  return section_triggers"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9XriqLyG76r"
      },
      "source": [
        "def get_question_triggers():\n",
        "  \"\"\"\n",
        "  no inputs\n",
        "  returns a list of strings that denote new sections in the document\n",
        "  \"\"\"\n",
        "\n",
        "  question_triggers = [\n",
        "    [],\n",
        "    [['Environmental Impact Total','Environmental Impacts Total',],\n",
        "      ['Does the species pose some hazard or threat to the health of native species',],\n",
        "      ['Does it outcompete native species for available resources',\n",
        "      'Does it out-compete native species for available resources',],\n",
        "      ['Does it alter predator-prey relationships?',],\n",
        "      ['Has it affected any native populations genetically',],\n",
        "      ['Does it negatively affect water quality',],\n",
        "      ['Does it alter the physical ecosystem in some way',\n",
        "      'Does it alter physical components of the ecosystem in some way',]],\n",
        "    [['Economic Impact Total',],\n",
        "      ['Does the species pose some hazard or threat to human health',],\n",
        "      ['Does it cause damage to infrastructure',],\n",
        "      ['Does it negatively affect water quality',],\n",
        "      ['Does it harm any markets or economic sectors',\n",
        "      'Does it negatively affect any markets or economic sectors',],\n",
        "      ['Does it inhibit recreational activities and/or associated tourism',],\n",
        "      ['Does it diminish the perceived aesthetic or natural value of the areas it inhabits',]],\n",
        "    [['Beneficial Effect Total','Positive Impact Total', 'Beneficial Impact Total',],\n",
        "      ['Does it act as a biological',\n",
        "      'Does it ac as a biological',],\n",
        "      ['Is it commercially valuable',],\n",
        "      ['Is it recreationally valuable',],\n",
        "      ['Does the species have some medicinal or research value',],\n",
        "      ['Does the species remove toxins or pollutants from the water or otherwise increase water quality',],\n",
        "      ['Does the species have a positive ecological impact outside of biological control',\n",
        "      'Does the species have a positive ecological effect outside of biological control']]\n",
        "  ]\n",
        "\n",
        "  return question_triggers"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74Xk7Gf0J8EZ"
      },
      "source": [
        "def first_char_bullet_point(impact_statement_line):\n",
        "  \"\"\"\n",
        "  inputs an impact statement line\n",
        "  checks if the first character of the impact statement line is a bullet point\n",
        "    if this is true, it indicates there is a new impact statement and \n",
        "    that this section is using bullet points to delineate impact statements\n",
        "  returns true or false of the condition\n",
        "  \"\"\"\n",
        "\n",
        "  if impact_statement_line[0] in ['•','●','']:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqLy8B0DJ_8F"
      },
      "source": [
        "def new_question(prev_impact_data, impact_data):\n",
        "  \"\"\"\n",
        "  inputs the previous impact data and the current impact data\n",
        "  checks if the current impact data relates to a new file,\n",
        "    new species, new impact section, or a new question as compared\n",
        "    to the previous impact data\n",
        "    if true, this indicates a new impact statement\n",
        "  returns true or false of the condition\n",
        "  \"\"\"\n",
        "\n",
        "  if impact_data[0] != prev_impact_data[0] or \\\n",
        "    impact_data[2] != prev_impact_data[2] or \\\n",
        "    impact_data[3] != prev_impact_data[3] or \\\n",
        "    impact_data[4] != prev_impact_data[4]:\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP2GEZBMKGFH"
      },
      "source": [
        "def prev_line_reference(prev_impact_statement_line, impact_statement_line, uses_bullets):\n",
        "  \"\"\"\n",
        "  inputs previous impact statement line, current impact statement line and \n",
        "    where the current impact statement is using bullets or not\n",
        "  if the current statement is not using bullets to delineate new statments\n",
        "    the function checks for the previous line ending in a reference and the \n",
        "    current line starting with a capital letter, indicating that the current\n",
        "    line is the beginning of a new impact statement\n",
        "  returns true or false of the condition\n",
        "  \"\"\"\n",
        "  \n",
        "  if not uses_bullets and \\\n",
        "    prev_impact_statement_line[-2:] == ').' and \\\n",
        "    impact_statement_line[0].isupper():\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3jsWra8KYWI"
      },
      "source": [
        "def last_character_hyphen(impact_line):\n",
        "  \"\"\"\n",
        "  inputs an impact line\n",
        "  checks if the last character of the impact line is a hyphen\n",
        "  returns true or false of the condition\n",
        "  \"\"\"\n",
        "\n",
        "  ends_with_hyphen = False\n",
        "  if len(impact_line) > 0:\n",
        "    if impact_line[-1] == '-':\n",
        "      ends_with_hyphen = True\n",
        "\n",
        "  return ends_with_hyphen"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFkmKFEcGsm_"
      },
      "source": [
        "## Extracting Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLau0JgaGwns"
      },
      "source": [
        "def exctract_impact_lines(pdf_file, data_path, output_file):\n",
        "  \"\"\"\n",
        "  inputs a pdf_file, the path of the file, and file object to output results\n",
        "  extracts all lines between impact questions for the impact sections\n",
        "    (Environmental, Socio-Economic, and Beneficial)\n",
        "  return None\n",
        "  \"\"\"\n",
        "\n",
        "  print('Extracting impact lines from ' + pdf_file)\n",
        "\n",
        "  with open(data_path + pdf_file, 'rb') as f:\n",
        "      pdf = pdftotext.PDF(f)\n",
        "\n",
        "      # Set variables for impact extractions\n",
        "      taxonomic_group, scientific_name, common_name = '', '', []\n",
        "      page_number, section, question = 0, 0, 0\n",
        "      section_triggers = get_section_triggers()\n",
        "      question_triggers = get_question_triggers()\n",
        "      impact_trigger, impact_statement = False, ['','','','','','']\n",
        "\n",
        "      # Set counters for monitoring section and question count\n",
        "      section_count = [0,0,0,0]\n",
        "      question_count = []\n",
        "      for i in range(4):\n",
        "        question_count.append([0,0,0,0,0,0,0])\n",
        "\n",
        "      for page_idx, page in enumerate(pdf):\n",
        "        page = clean_page(page)\n",
        "        lines = clean_lines(page)\n",
        "        lines = fix_document_errors(pdf_file, page_idx, lines)\n",
        "\n",
        "        for line_idx, line in enumerate(lines):\n",
        "          \n",
        "          if line_idx == len(lines)-1:\n",
        "            page_number = detect_page_number(line, page_number)\n",
        "\n",
        "          else:\n",
        "            taxonomic_group = detect_taxonimic_group(line, taxonomic_group)\n",
        "            scientific_name = detect_scientific_name(line, scientific_name, page_number)\n",
        "            common_name = detect_common_name(line, common_name)\n",
        "            section, impact_trigger = detect_section(line, section, section_triggers, section_count, scientific_name, impact_trigger)\n",
        "            question, impact_trigger = detect_question(line, section, question, question_triggers, question_count, impact_trigger)\n",
        "                                \n",
        "            if impact_trigger and len(line) > 0:\n",
        "              ordinal_number_warning(line)\n",
        "              output_file.write('{}|{}|{}|{}|{}|{}|{}\\n'.format(pdf_file, str(page_idx+1), section, question, scientific_name, common_name, line))\n",
        "\n",
        "            if question > 0 and scientific_name and 'Unknown' in line:\n",
        "              impact_trigger = True\n",
        "            \n",
        "  print(section_count)\n",
        "  print(question_count)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY9s44i2Gwrn"
      },
      "source": [
        "def locate_names_in_statements(scientific_names, common_names, common_to_scientific_dict, scientific_to_common_dict, common_name_occurrences_dict):\n",
        "  \"\"\"\n",
        "  inputs scientific name set, common name set, and names dictionary\n",
        "  searches impact statements for unique impacts on any species in the sets\n",
        "    (ignoring itself)\n",
        "  returns a set of impact relationship tuples\n",
        "  \"\"\"\n",
        "\n",
        "  impact_relationships = set()\n",
        "\n",
        "  with open(impact_statement_file, 'r') as impact_statements:\n",
        "    for impact_statement in impact_statements:\n",
        "      impact_statement = impact_statement.strip().lower()\n",
        "      if len(impact_statement) > 0: # ignore blank lines\n",
        "        impact_statement = impact_statement.split('|')\n",
        "        if impact_statement[-1] != 'impact_statement': #ignore headers\n",
        "          invasive_species = impact_statement[4].strip().lower()\n",
        "          statement = impact_statement[-1]\n",
        "\n",
        "          # search for scientific names\n",
        "          for scientific_name in scientific_names:\n",
        "            if scientific_name in statement and scientific_name != invasive_species:\n",
        "              frequent_invasive_name = frequent_common_from_scientific(invasive_species, scientific_to_common_dict, common_name_occurrences_dict)\n",
        "              frequent_impacted_name = frequent_common_from_scientific(scientific_name, scientific_to_common_dict, common_name_occurrences_dict)\n",
        "              if frequent_invasive_name != frequent_impacted_name:\n",
        "                impact_relationships.add((invasive_species, scientific_name, frequent_invasive_name,frequent_impacted_name))\n",
        "\n",
        "          # search for common names\n",
        "          potential_impacts = []\n",
        "          for common_name in common_names:\n",
        "            variations = [' '+common_name+' ', '|'+common_name+' ', ' '+common_name+'.', ' '+common_name+',']\n",
        "            for variation in variations:\n",
        "              if variation in statement:\n",
        "                if invasive_species not in common_to_scientific_dict[common_name]:\n",
        "                  frequent_invasive_name = frequent_common_from_scientific(invasive_species, scientific_to_common_dict, common_name_occurrences_dict)\n",
        "                  if frequent_invasive_name != common_name:\n",
        "                    potential_impacts.append(common_name)\n",
        "\n",
        "          # remove overlapping names (ex: \"brown bullhead\" would yield \"brown bullhead\" and \"bullhead\")\n",
        "          potential_impact_list = potential_impacts.copy()\n",
        "          for potential_impact in potential_impact_list:\n",
        "            for check_impact in potential_impact_list:\n",
        "              if (potential_impact in check_impact) and (potential_impact != check_impact):\n",
        "                if potential_impact in potential_impacts:\n",
        "                  potential_impacts.remove(potential_impact)\n",
        "\n",
        "          # add non-overlapping names to relationship set\n",
        "          for potential_impact in potential_impacts:\n",
        "            impact_relationships.add((invasive_species, potential_impact, frequent_invasive_name, potential_impact))\n",
        "\n",
        "  print('{} relationships extracted from impact statements'.format(len(impact_relationships)))\n",
        "\n",
        "  return impact_relationships"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1an-czFoGwvC"
      },
      "source": [
        "def locate_related_names_in_statements(invasive_species, statement, scientific_to_common_dict, related_species_names_dict, common_name_occurrences_dict, impact_relationships):\n",
        "  \"\"\"\n",
        "  inputs invasive species, statement and dictionaries: scientific to common name, related species names, common_name_occurrences\n",
        "  searches impact statements for relationships and adds them the to impact relationships set\n",
        "  returns a set of impact relationship tuples\n",
        "  \"\"\"\n",
        "\n",
        "  related_names = set(related_species_names_dict.keys())\n",
        "  for related_name in related_names:\n",
        "    if related_name[:-1] in statement:\n",
        "      if invasive_species in related_names:\n",
        "        frequent_invasive_name = related_species_names_dict[invasive_species]\n",
        "      else:\n",
        "        frequent_invasive_name = frequent_common_from_scientific(invasive_species, scientific_to_common_dict, common_name_occurrences_dict)\n",
        "      if frequent_invasive_name + 's' in related_names:\n",
        "        frequent_invasive_name += 's'\n",
        "        \n",
        "      noaa_impacted_name = related_species_names_dict[related_name]\n",
        "      if frequent_invasive_name != noaa_impacted_name:\n",
        "        impact_relationships.add((invasive_species, related_name, frequent_invasive_name, noaa_impacted_name))\n",
        "\n",
        "  return impact_relationships"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvBjK8qdGwwn"
      },
      "source": [
        "def extract_impacted_names_from_statements(impact_statement, invasive_species, name_search, related_species_names_dict, common_to_scientific_dict):\n",
        "  \"\"\"\n",
        "  inputs impact statement, invasive species, name search and related species name dict, common_to_scientific_dict\n",
        "  searches impact statements for impacted species\n",
        "  returns a list of impacted species\n",
        "  \"\"\"\n",
        "\n",
        "  impacted_names = []\n",
        "  impact_statement = impact_statement.lower().strip()\n",
        "\n",
        "  for name in name_search:\n",
        "    if len(name) > 4:\n",
        "      if name in impact_statement:\n",
        "        impacted_names.append(name)\n",
        "      elif name[-1] == 's' and name[:-1] in impact_statement:\n",
        "        impacted_names.append(name)\n",
        "  \n",
        "  #filter out sub names (ex: carp and grass carp)\n",
        "  temp_names = impacted_names\n",
        "  impacted_names = []\n",
        "  for name_idx, name in enumerate(temp_names):\n",
        "    sub_name = False\n",
        "    for other_idx, other_name in enumerate(temp_names):\n",
        "      if name in other_name and name_idx != other_idx:\n",
        "        sub_name = True\n",
        "    if sub_name == False:\n",
        "      related_name = related_species_names_dict.get(invasive_species, invasive_species) \n",
        "      if related_name != related_species_names_dict.get(name, name):\n",
        "        if invasive_species not in common_to_scientific_dict.get(name, name):\n",
        "          impacted_names.append(name)\n",
        "\n",
        "  return impacted_names"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WY-wX3VONsJ"
      },
      "source": [
        "def extract_impact_relationships():\n",
        "  \"\"\"\n",
        "  no inputs\n",
        "  loads the scientific name and common name dictionary from a file,\n",
        "    adds the names from the impact statements,\n",
        "    searches through impact statements to find relationships between species\n",
        "    writes relationships to file\n",
        "  returns None\n",
        "  \"\"\"\n",
        "\n",
        "  print('Extracting impact relationships between species...')\n",
        "  related_species_names_dict = create_related_species_names_dict()\n",
        "  common_to_scientific_dict = create_common_to_scientific_dict(waterlife_data_excel_file)\n",
        "  common_to_scientific_dict = add_impact_statement_names_to_dict(common_to_scientific_dict)  \n",
        "  common_name_occurrences_dict = count_occurences(common_to_scientific_dict)\n",
        "  scientific_to_common_dict = invert_dict(common_to_scientific_dict)\n",
        "  invasive_species_ids = get_species_ids()\n",
        "  species_id_dict = create_species_id_dict(related_species_names_dict, scientific_to_common_dict, common_name_occurrences_dict, invasive_species_ids)\n",
        "  impact_relationships = create_impact_relationships(scientific_to_common_dict, related_species_names_dict, common_name_occurrences_dict, species_id_dict)\n",
        "  write_impact_relationships_to_file(impact_relationships)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xs6wYDAOe6t3"
      },
      "source": [
        "def extract_journal_references():\n",
        "  \"\"\"\n",
        "  inputs nothing\n",
        "  extracts references at end of each file, write refs to a file\n",
        "  returns None\n",
        "  \"\"\"\n",
        "\n",
        "  print('Extracting references...')\n",
        "\n",
        "  output_file = open(references_file, 'w')\n",
        "  output_file.write('pdf_file|reference\\n')  \n",
        "  \n",
        "  ref_trigger_dict = {\n",
        "    'tm-161.pdf': 'APPENDIX B. OIA REFERENCES',\n",
        "    'tm-161b.pdf': '3.0 LITERATURE CITED',\n",
        "    'tm-161c.pdf': '4.0 REFERENCES',\n",
        "    'tm-169.pdf': 'APPENDIX B: Literature Cited in Assessments',\n",
        "    'tm-169b.pdf': '4.0 REFERENCES',\n",
        "    'tm-169c.pdf': '5.0 REFERENCES',\n",
        "  }\n",
        "\n",
        "  pdf_files = os.listdir(assets_path)\n",
        "  for pdf_file in pdf_files:\n",
        "    if pdf_file[-4:] == '.pdf':\n",
        "      if pdf_file in ref_trigger_dict.keys():\n",
        "\n",
        "        in_ref_section = False\n",
        "        ref_title =  ref_trigger_dict[pdf_file]\n",
        "        ref_counter = 0\n",
        "        page_number = 0\n",
        "        ref = ''\n",
        "\n",
        "        with open(assets_path + pdf_file, 'rb') as f:\n",
        "            pdf = pdftotext.PDF(f)\n",
        "\n",
        "            for page_idx, page in enumerate(pdf):\n",
        "              page = clean_page(page)\n",
        "              lines = clean_lines(page)\n",
        "              lines = fix_document_errors(pdf_file, page_idx, lines)\n",
        "\n",
        "              for line_idx, line in enumerate(lines):\n",
        "                \n",
        "                if line_idx == len(lines)-1:\n",
        "                  page_number = detect_page_number(line, page_number)\n",
        "                else:\n",
        "                  if in_ref_section:\n",
        "\n",
        "                    new_line_ord = ord(line[0])\n",
        "                    old_line_ord = old_line_ordinal(ref)\n",
        "\n",
        "                    if line[0].isupper() and\\\n",
        "                      ('pp.' in ref or\\\n",
        "                        'http:' in ref or\\\n",
        "                        'www.' in ref or\\\n",
        "                        'fact sheet' in ref.lower() or\\\n",
        "                        'in press' in ref.lower() or\\\n",
        "                        'booklet' in ref.lower() or\\\n",
        "                        'available' in ref.lower() or\\\n",
        "                        'accessed' in ref.lower() or\\\n",
        "                        'annual report.' in ref.lower() or\\\n",
        "                        'vienna.' in ref.lower() or\\\n",
        "                        'report.' in ref.lower() or\\\n",
        "                        'izdatel' in ref.lower() or\\\n",
        "                        'doi:' in ref.lower() or\\\n",
        "                        'hokkaido university press.' in ref.lower() or\\\n",
        "                        'synopsis 135.' in ref.lower() or\\\n",
        "                        ref[-7:] == '(eds.).' or\\\n",
        "                        ref[-9:] == '(GLMRIS).' or\\\n",
        "                        len(prev_line) <= 80 or\\\n",
        "                        bool(re.search(r'\\(\\d\\d\\d\\d[a-z]?\\).?$', prev_line)) or\\\n",
        "                        (bool(re.search(r'[A-Z]{2}.$', prev_line)) and prev_line[-4:] != 'USA.') or\\\n",
        "                        ((bool(re.search(r'\\d(-|–)\\d', ref)) or\\\n",
        "                          bool(re.search(r'\\):\\d', ref)) or\\\n",
        "                          bool(re.search(r'\\d:\\d', ref))) and\\\n",
        "                          ref[-1] == '.')) and\\\n",
        "                      (new_line_ord >= old_line_ord) and\\\n",
        "                      prev_line[-4:] != 'with' and\\\n",
        "                      prev_line[-4:] != 'Nauk' and\\\n",
        "                      prev_line[-4:] != 'tate' and\\\n",
        "                      prev_line[-4:] != 'Lake' and\\\n",
        "                      prev_line[-4:] != '. B.' and\\\n",
        "                      prev_line[-4:] != '. R.' and\\\n",
        "                      prev_line[-5:] != 'le at' and\\\n",
        "                      prev_line[-4:] != 'NOAA' and\\\n",
        "                      prev_line[-4:] != 'USGS' and\\\n",
        "                      prev_line[-4:] != '(ECI' and\\\n",
        "                      prev_line[-11:] != '08/18/2017.' and\\\n",
        "                      prev_line[-5:] != '1990.' and\\\n",
        "                      prev_line[-2:] != 'of' and\\\n",
        "                      prev_line[-12:] != 'DFO Canadian' and\\\n",
        "                      prev_line[-12:] != 'Case report.' and\\\n",
        "                      prev_line[-8:] != '50_7.pdf' and\\\n",
        "                      prev_line[-9:] != 'phoxinus.' and\\\n",
        "                      prev_line[-8:] != 'European' and\\\n",
        "                      prev_line[-8:] != 'Y, 2009.' and\\\n",
        "                      prev_line[-10:] != '(Accessed:' and\\\n",
        "                      prev_line[-39:] != 'bodies. Qi, J., and K.T. Evered (eds.).' and\\\n",
        "                      prev_line[-6:] != '3. St.':\n",
        "\n",
        "                      if len(ref) > 0:\n",
        "                        output_file.write('{}|{}\\n'.format(pdf_file, ref))\n",
        "                        ref_counter += 1\n",
        "                      \n",
        "                      ref = line.strip()\n",
        "\n",
        "                    else:\n",
        "                      if len(ref) > 0:\n",
        "                        if ref[-1] == '-':\n",
        "                          ref = ref.strip() + line.strip()\n",
        "                        else:\n",
        "                          ref = ref.strip() + ' ' + line.strip()  \n",
        "                      else:\n",
        "                        ref = ref.strip() + ' ' + line.strip()\n",
        "\n",
        "                  \n",
        "                  if not in_ref_section and line == ref_title:\n",
        "                    in_ref_section = True\n",
        "                  prev_line = line\n",
        "\n",
        "        output_file.write('{}|{}\\n'.format(pdf_file, ref))\n",
        "        ref_counter += 1\n",
        "        print('{} references extracted from {}'.format(ref_counter, pdf_file))\n",
        "\n",
        "  output_file.close()\n",
        "\n",
        "  return None"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mELjK_AgdfC"
      },
      "source": [
        "def extract_impact_references():\n",
        "  \"\"\"\n",
        "  inputs None\n",
        "  finds citations within the document and matchres them to the journal reference\n",
        "  returns None\n",
        "  \"\"\"\n",
        "  print('Extracting references...')\n",
        "\n",
        "  related_species_names_dict = create_related_species_names_dict()\n",
        "  common_to_scientific_dict = create_common_to_scientific_dict(waterlife_data_excel_file)\n",
        "  common_to_scientific_dict = add_impact_statement_names_to_dict(common_to_scientific_dict)  \n",
        "\n",
        "  name_search = set()\n",
        "  for name in related_species_names_dict.keys():\n",
        "    name_search.add(name)\n",
        "  for name in common_to_scientific_dict.keys():\n",
        "    name_search.add(name)\n",
        "\n",
        "  counter = 0\n",
        "  impact_type_dict = make_impact_type_dict()\n",
        "  output_file = open(database_upload_file, 'w')\n",
        "  output_file.write('ID|species_ID|impact_type|study_type|study_location|impact_desc|refnum|notes|greatlakes_region|cost|location|impacted_TSNs\\n')  \n",
        "  \n",
        "  # read through impact statement file to extract references\n",
        "  with open(impact_statement_file, 'r') as impact_statements:\n",
        "    for impact_statement in impact_statements:\n",
        "      impact_statement = impact_statement.strip()\n",
        "      if len(impact_statement) > 0: # ignore blank lines\n",
        "        impact_statement = impact_statement.split('|')\n",
        "        if impact_statement[-1] != 'impact_statement': #ignore headers\n",
        "          impact_file = impact_statement[0].strip().lower()\n",
        "          impact_section = impact_statement[2].strip()\n",
        "          impact_question = impact_statement[3].strip()\n",
        "          impact_section_question = impact_section + '|' + impact_question\n",
        "          invasive_species = impact_statement[4].strip()     \n",
        "          statement = impact_statement[-1].strip().lower()\n",
        "          statement = statement.replace('.','')\n",
        "          statement = statement.replace(',','')\n",
        "          statement = statement.replace('(','')\n",
        "          statement = statement.replace(')','')\n",
        "          words = statement.split(' ')\n",
        "          for word_index, word in enumerate(words):\n",
        "            if bool(re.search(r'^\\d\\d\\d\\d', word)):\n",
        "              year = word[:4]\n",
        "              author = words[word_index-1]\n",
        "              if author == 'al':\n",
        "                author = words[word_index-3]\n",
        "\n",
        "              if author not in ['in','and','of', 'to', 'a', 'the', 'from']:\n",
        "\n",
        "                with open(references_file, 'r') as references:\n",
        "                  potential_references = []\n",
        "                  for reference in references:\n",
        "                    reference_file, clean_reference = reference.split('|')\n",
        "                    clean_reference = clean_reference.strip().lower()\n",
        "                    if year in clean_reference and author in clean_reference and reference_file == impact_file:\n",
        "                      potential_references.append((clean_reference.find(author), reference))\n",
        "                \n",
        "                if len(potential_references) > 0:\n",
        "                  for potential_reference_index, potential_reference in enumerate(potential_references):\n",
        "                    if potential_reference_index == 0:\n",
        "                      min_index = 0\n",
        "                      min_location = potential_reference[0]\n",
        "                    if potential_reference[0] < min_location:\n",
        "                      min_index = potential_reference_index\n",
        "                      min_location = potential_reference[0]\n",
        "                \n",
        "                  #print('selected {}'.format(potential_references[min_index][1]))\n",
        "                  id = counter\n",
        "                  species_id = invasive_species\n",
        "                  impact_type = impact_type_dict[impact_section_question]\n",
        "                  study_type = ''\n",
        "                  study_location = ''\n",
        "                  impact_desc = impact_statement[-1]\n",
        "                  refnum = potential_references[min_index][1].split('|')[1].strip()\n",
        "                  notes = ''\n",
        "                  greatlakes_region = 1\n",
        "                  cost = ''\n",
        "                  location = ''\n",
        "                  impacted_TSNs = extract_impacted_names_from_statements(impact_desc, invasive_species.lower().strip(), name_search, related_species_names_dict, common_to_scientific_dict)\n",
        "\n",
        "                  output_file.write('{}|{}|{}|{}|{}|{}|{}|{}|{}|{}|{}|{}\\n'.format(\n",
        "                      id,species_id,impact_type,study_type,study_location,impact_desc, \\\n",
        "                      refnum,notes,greatlakes_region,cost,location,impacted_TSNs))  \n",
        "\n",
        "                  counter+=1\n",
        "  \n",
        "  print('{} impact statements matched to references.'.format(counter))\n",
        "\n",
        "  output_file.close()\n",
        "\n",
        "  return None"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6Ig1q9_GxJt"
      },
      "source": [
        "## Processing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qroY_XgRGzG7"
      },
      "source": [
        "def ordinal_number_warning(line):\n",
        "  \"\"\"\n",
        "  inputs an impact statement line\n",
        "  detects ordinal numbers (1st, 2nd, 3rd, 4th etc..) that did not parse correctly\n",
        "    prints a warning to the screen if an ordinal number is detected\n",
        "  returns None\n",
        "  \"\"\"\n",
        "\n",
        "  if (line == 'st' or line == 'nd' or line == 'rd' or line =='th'):\n",
        "    print('Warning possible ordinal number detected for scientific_name {} on page {}'.format(scientific_name, str(page_idx+1)))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zo7Ih2MgGzMF"
      },
      "source": [
        "def build_statement_break_classifier():\n",
        "  \"\"\"\n",
        "  no inputs\n",
        "  opens the impact statement line file and finds all known statement break lines\n",
        "    and non statement break lines based on bullet points, new questions and \n",
        "    references at the end of the line, then creates trains and a classifier based\n",
        "    on this data to be used for impact statement lines where it is unknown if\n",
        "    the impact statement line represents a new impact statement\n",
        "  returns the trained classifier\n",
        "  \"\"\"\n",
        "\n",
        "  print('Training statement break classifier')\n",
        "  X_train = []\n",
        "  y_train = []\n",
        "  counter = 0\n",
        "  prev_impact_data = ['','','','','','']\n",
        "  uses_bullets = False\n",
        "  statement_break_classifier = GaussianNB()\n",
        "\n",
        "  with open(impact_line_file, 'r') as impact_lines:\n",
        "    for impact_line in impact_lines:\n",
        "\n",
        "      impact_line = impact_line.strip()\n",
        "      impact_data = impact_line.split('|')\n",
        "      new_statement = False\n",
        "      impact_statement_line = impact_data[-1].strip()\n",
        "      prev_impact_statement_line = prev_impact_data[-1].strip()\n",
        "      last_line_len = len(prev_impact_statement_line)\n",
        "\n",
        "      if first_char_bullet_point(impact_statement_line): # first character is a bullet point\n",
        "        new_statement, uses_bullets = True, True\n",
        "      elif new_question(prev_impact_data, impact_data): # new question in file\n",
        "        new_statement, uses_bullets = True, False\n",
        "      elif prev_line_reference(prev_impact_statement_line, impact_statement_line, uses_bullets):\n",
        "        new_statement = True\n",
        "\n",
        "      if new_statement and prev_impact_data != ['','','','','','']:\n",
        "        X_train.append(last_line_len)\n",
        "        y_train.append(1)\n",
        "      else:\n",
        "        if uses_bullets:\n",
        "          X_train.append(last_line_len)\n",
        "          y_train.append(0)\n",
        "\n",
        "      prev_impact_data = impact_data\n",
        "  \n",
        "  statement_break_classifier.fit(np.array(X_train).reshape(-1, 1), y_train)\n",
        "  \n",
        "  return statement_break_classifier"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8E5Yr5UGzP4"
      },
      "source": [
        "def prev_line_prediction(prev_impact_statement_line, impact_statement_line, uses_bullets, statement_break_classifier):\n",
        "  \"\"\"\n",
        "  inputs previous impact statement line, current impact statement line and \n",
        "    where the current impact statement is using bullets or not and the\n",
        "    statement break classifier that was trained on known data\n",
        "  if the current statement is not using bullets to delineate new statments\n",
        "    the function checks for the classifiers prediction of whether the previous line\n",
        "    is the end of an impact statement and whether the current line starts with\n",
        "    a capital letter, thus indication that the current line is the beginning \n",
        "    of a new impact statement\n",
        "  returns true or false of the condition\n",
        "  \"\"\"\n",
        "  \n",
        "  if not uses_bullets and \\\n",
        "    impact_statement_line[0].isupper() and \\\n",
        "    statement_break_classifier.predict(np.array(len(prev_impact_statement_line)).\n",
        "      reshape(-1,1)) == 1:\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dct13j9UKSdX"
      },
      "source": [
        "def detect_statement_breaks(prev_impact_data, impact_data, uses_bullets, statement_break_classifier):\n",
        "  \"\"\"\n",
        "  inputs the previous impact data and the current impact data as lists\n",
        "  determines if the impact statement line belongs to the same section and question\n",
        "  returns a boolean value regarding whether the current line is associated with the \n",
        "    previous line, and boolean value regarding wether the current question uses\n",
        "    bullet points to delineate impact statements\n",
        "  \"\"\"\n",
        "\n",
        "  new_statement = False\n",
        "  impact_statement_line = impact_data[-1].strip()\n",
        "  prev_impact_statement_line = prev_impact_data[-1].strip()\n",
        "\n",
        "  if first_char_bullet_point(impact_statement_line): # first character is a bullet point\n",
        "    new_statement, uses_bullets = True, True\n",
        "  elif new_question(prev_impact_data, impact_data): # new question in file\n",
        "    new_statement, uses_bullets = True, False\n",
        "  elif prev_line_reference(prev_impact_statement_line, impact_statement_line, uses_bullets):\n",
        "    new_statement = True\n",
        "  elif prev_line_prediction(prev_impact_statement_line, impact_statement_line, uses_bullets, statement_break_classifier):\n",
        "    new_statement = True\n",
        "\n",
        "  return new_statement, uses_bullets"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAVN1uXoKU8W"
      },
      "source": [
        "def remove_bullet_point(impact_line):\n",
        "  \"\"\"\n",
        "  inputs an impact line\n",
        "  removes the leading bullet point if any\n",
        "  returns the impact line without the leading bullet point\n",
        "  \"\"\"\n",
        "\n",
        "  impact_line = impact_line.strip()\n",
        "  if first_char_bullet_point(impact_line):\n",
        "    impact_line = impact_line[1:].strip()\n",
        "\n",
        "  return impact_line"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wWDf3-wKdJA"
      },
      "source": [
        "def aggregate_impact_statements(statement_break_classifier):\n",
        "  \"\"\"\n",
        "  no inputs\n",
        "  reads the impact lines txt file and aggregates lines into statements\n",
        "    removes lines that are not statments\n",
        "  writes data to impact statements txt file\n",
        "  \"\"\"\n",
        "\n",
        "  counter = 0\n",
        "  prev_impact_data = ['','','','','','']\n",
        "  uses_bullets = False\n",
        "  ends_with_hyphen = False\n",
        "\n",
        "  output_file = open(impact_statement_file, 'w')\n",
        "\n",
        "  with open(impact_line_file, 'r') as impact_lines:\n",
        "    for impact_line in impact_lines:\n",
        "\n",
        "      impact_line = impact_line.strip()\n",
        "      impact_data = impact_line.split('|')\n",
        "      new_statement, uses_bullets = detect_statement_breaks(prev_impact_data, impact_data, uses_bullets, statement_break_classifier)\n",
        "      impact_data[-1] = remove_bullet_point(impact_data[-1])\n",
        "\n",
        "      if new_statement:\n",
        "        output_file.write('\\n'+ '|'.join(impact_data))\n",
        "      else:\n",
        "        if not ends_with_hyphen:\n",
        "          output_file.write(' ')\n",
        "        output_file.write(impact_data[-1])\n",
        "\n",
        "      ends_with_hyphen = last_character_hyphen(impact_data[-1])\n",
        "      prev_impact_data = impact_data\n",
        "\n",
        "      if new_statement:\n",
        "        counter += 1\n",
        "  \n",
        "  print('{} impact statements extracted'.format(counter))\n",
        "  print('Impact statements file created')\n",
        "  \n",
        "  output_file.close()\n",
        "\n",
        "  return None"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olRtWIQ8OJzS"
      },
      "source": [
        "def create_impact_relationships(scientific_to_common_dict, related_species_names_dict, common_name_occurrences_dict, species_id_dict):\n",
        "  \"\"\"\n",
        "  inputs the related species names, scientific to common, common name occurrences and species_id dictionaries\n",
        "  loads the noaa impact statement and scraped impact statement files and creates a relationships between spcies\n",
        "  returs the impact relationship set\n",
        "  \"\"\"\n",
        "\n",
        "  impact_relationships = set()\n",
        "\n",
        "  # extract relationships from noaa statement file\n",
        "  df = pd.DataFrame(pd.read_csv(old_impact_statements_file))\n",
        "  for i, row in df.iterrows():\n",
        "    if row[8] == 1: # great lakes region only\n",
        "      try:\n",
        "        idx = int(str(row[1]).strip())\n",
        "        if idx in species_id_dict.keys():\n",
        "          invasive_species = species_id_dict[idx]\n",
        "          statement = row[5].strip().lower()\n",
        "          impact_relationships = locate_related_names_in_statements(invasive_species, statement, scientific_to_common_dict, related_species_names_dict, common_name_occurrences_dict, impact_relationships)\n",
        "      except:\n",
        "        pass\n",
        "\n",
        "  # extract relationships from impact statement file\n",
        "  with open(impact_statement_file, 'r') as impact_statements:\n",
        "    for impact_statement in impact_statements:\n",
        "      impact_statement = impact_statement.strip().lower()\n",
        "      if len(impact_statement) > 0: # ignore blank lines\n",
        "        impact_statement = impact_statement.split('|')\n",
        "        if impact_statement[-1] != 'impact_statement': #ignore headers\n",
        "          invasive_species = impact_statement[4].strip().lower()\n",
        "          statement = impact_statement[-1]\n",
        "          impact_relationships = locate_related_names_in_statements(invasive_species, statement, scientific_to_common_dict, related_species_names_dict, common_name_occurrences_dict, impact_relationships)\n",
        "\n",
        "  print('{} relationships extracted from impact statements'.format(len(impact_relationships)))\n",
        "\n",
        "  return impact_relationships"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeD1LL1yezsv"
      },
      "source": [
        "def old_line_ordinal(ref):\n",
        "  \"\"\"\n",
        "  inputs the reference statement that is being built\n",
        "  compute the ordinal value of the first character (used for finding new references assuming alphabetical order)\n",
        "    adjusts for letters with accents and references that are not in alphabetical order\n",
        "  returns the ordinal value\n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    old_line_ord = ord(ref[0])\n",
        "  except:\n",
        "    old_line_ord = 0\n",
        "\n",
        "  if old_line_ord == 70:\n",
        "    if ref[:5] == 'Fogel': # out of order in biblio\n",
        "      old_line_ord = 67\n",
        "  elif old_line_ord == 72: \n",
        "    if ref[:5] == 'Horns': # out of order in biblio\n",
        "      old_line_ord = 69\n",
        "  elif old_line_ord == 84: \n",
        "    if ref[:11] == 'Tinca tinca': # out of order in biblio\n",
        "      old_line_ord = 65\n",
        "  elif old_line_ord == 193: # fix A with accent\n",
        "    old_line_ord = 65\n",
        "  elif old_line_ord == 199: # fix C with accent\n",
        "    old_line_ord = 67\n",
        "  elif old_line_ord == 214: # fix O with accent\n",
        "    old_line_ord = 79\n",
        "  elif old_line_ord == 216: # fix O with accent\n",
        "    old_line_ord = 79\n",
        "  elif old_line_ord == 262: # fix C with accent\n",
        "    old_line_ord = 67\n",
        "  elif old_line_ord == 352: # fix S with accent\n",
        "    old_line_ord = 83\n",
        "  elif old_line_ord == 381: # fix S with accent\n",
        "    old_line_ord = 90\n",
        "\n",
        "  return old_line_ord"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3REuTllF1-H"
      },
      "source": [
        "## Writing Output Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fZuaQPyFgIx"
      },
      "source": [
        "def print_header(output_file):\n",
        "  \"\"\"\n",
        "  no inputs\n",
        "  prints the header for the output\n",
        "  returns none\n",
        "  \"\"\"\n",
        "\n",
        "  output_file.write('pdf_file|page|section|question|scientific_name|common_name|impact_statement\\n')\n",
        "  return None"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtMe5PWhgZEA"
      },
      "source": [
        "def make_impact_type_dict():\n",
        "  \"\"\"\n",
        "  no inputs\n",
        "  creates a dictionary based on NOAA's impact types from their prior impact statements and\n",
        "    relates them to the section and question in the new documents\n",
        "  return impact type dictionary\n",
        "  \"\"\"\n",
        "  impact_type_dict = {\n",
        "      '1|1' : 'Disease/Parasites/Toxicity',\n",
        "      '1|2' : 'Competition',\n",
        "      '1|3' : 'Predation/Herbivory',\n",
        "      '1|4' : 'Genetic',\n",
        "      '1|5' : 'Water Quality',\n",
        "      '1|6' : 'Habitat Alteration',\n",
        "      '2|1' : 'Human Health',\n",
        "      '2|2' : 'Infrastructure',\n",
        "      '2|3' : 'Water Quality',\n",
        "      '2|4' : 'Commerce',\n",
        "      '2|5' : 'Recreation',\n",
        "      '2|6' : 'Property Value',\n",
        "      '3|1' : 'Other',\n",
        "      '3|2' : 'Aquaculture/Agriculture',\n",
        "      '3|3' : 'Recreation',\n",
        "      '3|4' : 'Other',\n",
        "      '3|5' : 'Water Quality',\n",
        "      '3|6' : 'Other',\n",
        "  }\n",
        "  return impact_type_dict"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNQ-eXeMFgXs"
      },
      "source": [
        "def create_impact_line_file():\n",
        "  \"\"\"\n",
        "  No inputs\n",
        "  Creates a file output object\n",
        "    Iterates through all files in the data folder\n",
        "    Sends each file to the extraction function\n",
        "  Returns None\n",
        "  \"\"\"\n",
        "\n",
        "  output_file = open(impact_line_file, 'w')\n",
        "\n",
        "  print_header(output_file)\n",
        "\n",
        "  pdf_files = os.listdir(assets_path)\n",
        "  for pdf_file in pdf_files:\n",
        "    if pdf_file[-4:] == '.pdf':\n",
        "      exctract_impact_lines(pdf_file, assets_path, output_file)\n",
        "\n",
        "  output_file.close()\n",
        "\n",
        "  print('Impact line file created')"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hw4O25eFgfV"
      },
      "source": [
        "def write_impact_relationships_to_file(impact_relationships):\n",
        "  \"\"\"\n",
        "  inputs the set of impact relationship tuples\n",
        "  writes weach tuple to a line in the impact relationship file\n",
        "  returns None  \n",
        "  \"\"\"\n",
        "\n",
        "  output_file = open(impact_relationships_file, 'w')\n",
        "  output_file.write('tex_invasive|text_impacted|translated_invasive|translated_impacted\\n')\n",
        "  for (text_invasive, text_impacted, tranlsated_invasive, translated_impacted) in impact_relationships:\n",
        "    output_file.write('{}|{}|{}|{}\\n'.format(text_invasive, text_impacted, tranlsated_invasive, translated_impacted))\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAldAL_uLYeQ"
      },
      "source": [
        "## Creating Taxonomy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJReyPphLawp"
      },
      "source": [
        "def create_common_to_scientific_dict(waterlife_data_excel_file):\n",
        "  \"\"\"\n",
        "  inputs the waterlife data file\n",
        "  creates a dictionary based on the waterlife data file\n",
        "  returns the dictionary\n",
        "  \"\"\"\n",
        "  \n",
        "  common_to_scientific_dict = dict()\n",
        "  df = pd.DataFrame(pd.read_excel(waterlife_data_excel_file))\n",
        "  df = df.iloc[:, 0:2].dropna()\n",
        "  for i, row in df.iterrows():\n",
        "    scientific_name = row[0].strip().lower()\n",
        "    common_names = row[1].strip().lower().split('/')\n",
        "    if len(scientific_name) > 0:\n",
        "      for i, common_name in enumerate(common_names):\n",
        "        if i >= 0: #if i == 0:\n",
        "          common_name = common_name.strip()\n",
        "          if common_name in common_to_scientific_dict.keys():\n",
        "            common_to_scientific_dict[common_name].append(scientific_name)\n",
        "          else:\n",
        "            common_to_scientific_dict[common_name] = [scientific_name]\n",
        "            \n",
        "  return common_to_scientific_dict"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5d6c9ROLa1W"
      },
      "source": [
        "def clean_common_name(common_name):\n",
        "  \"\"\"\n",
        "  inputs a common name\n",
        "  lowers, strips and removes leading 'a ' and leading 'an ' from common name\n",
        "  returns cleaned common name\n",
        "  \"\"\"\n",
        "\n",
        "  common_name = common_name.lower().strip()\n",
        "  common_name = common_name.replace(\"'\",'')    \n",
        "  if common_name[0:2] == 'a ':\n",
        "    common_name = common_name[2:]    \n",
        "  if common_name[0:3] == 'an ':\n",
        "    common_name = common_name[3:]\n",
        "  common_name = common_name.lower().strip()\n",
        "  \n",
        "  return common_name"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCR5FW25La6W"
      },
      "source": [
        "def add_impact_statement_names_to_dict(common_to_scientific_dict):\n",
        "  \"\"\"\n",
        "  inputs the name_dict dictionary\n",
        "  iterates through the impact statements to extract all scientific names\n",
        "    and their related common names\n",
        "    adds these mappings to the dictionary\n",
        "  returns an updated name_dict\n",
        "  \"\"\"\n",
        "\n",
        "  with open(impact_statement_file, 'r') as impact_statements:    \n",
        "    for impact_statement in impact_statements:\n",
        "      impact_statement = impact_statement.strip()      \n",
        "      if len(impact_statement) > 0: # ignore blank lines\n",
        "        impact_statement = impact_statement.split('|')        \n",
        "        if impact_statement[-1] != 'impact_statement': #ignore headers\n",
        "          invasive_species = impact_statement[4].strip().lower()\n",
        "          common_name = impact_statement[5]\n",
        "          common_name = common_name[1:-1]\n",
        "          common_name_list = common_name.split(',')          \n",
        "          for common_name in common_name_list:\n",
        "            common_name = clean_common_name(common_name)            \n",
        "            if len(common_name) > 0:\n",
        "              if common_name not in common_to_scientific_dict.keys():\n",
        "                common_to_scientific_dict[common_name] = [invasive_species]            \n",
        "              else:\n",
        "                if invasive_species not in common_to_scientific_dict[common_name]:\n",
        "                  common_to_scientific_dict[common_name].append(invasive_species)\n",
        "  \n",
        "  return common_to_scientific_dict"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJw65qUZLa9P"
      },
      "source": [
        "def count_occurences(common_to_scientific_dict):\n",
        "  \"\"\"\n",
        "  inputs common to scientific dictionary\n",
        "  goes through all the text extracted from NOAA's technical memorandum and counts\n",
        "    which names are used most often in the text and stores that in a dictionary\n",
        "  outputs common name occurrences dictionary\n",
        "  \"\"\"\n",
        "  common_names = common_to_scientific_dict.keys()\n",
        "  common_name_occurrences_dict = dict()\n",
        "  for common_name in common_names:\n",
        "    common_name_occurrences_dict[common_name] = 0\n",
        "  with open(impact_statement_file, 'r') as impact_statements:\n",
        "    for impact_statement in impact_statements:\n",
        "      impact_statement = impact_statement.strip().lower()\n",
        "      if len(impact_statement) > 0: # ignore blank lines\n",
        "        impact_statement = impact_statement.split('|')\n",
        "        if impact_statement[-1] != 'impact_statement': #ignore headers\n",
        "          statement = impact_statement[-1]\n",
        "          for common_name in common_names:\n",
        "            if common_name not in ['pink']: # too common\n",
        "              variations = [' '+common_name+' ', '|'+common_name+' ', ' '+common_name+'.', ' '+common_name+',']\n",
        "              for variation in variations:\n",
        "                if variation in statement:\n",
        "                    common_name_occurrences_dict[common_name] += 1\n",
        "  \n",
        "\n",
        "  return common_name_occurrences_dict"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q2TflSyND2T"
      },
      "source": [
        "def invert_dict(common_to_scientific_dict):\n",
        "  \"\"\"\n",
        "  inputs the common_name to scientific_name dictionary\n",
        "  inverts the dictionary which is a many to many relationship\n",
        "  returns the inverted dict\n",
        "  \"\"\"\n",
        "\n",
        "  scientific_to_common_dict = dict()\n",
        "\n",
        "  common_names = common_to_scientific_dict.keys()\n",
        "\n",
        "  for common_name in common_names:\n",
        "    scientific_names = common_to_scientific_dict[common_name]\n",
        "    for scientific_name in scientific_names:\n",
        "      if len(scientific_name) > 0:\n",
        "        if scientific_name in scientific_to_common_dict.keys():\n",
        "          scientific_to_common_dict[scientific_name].append(common_name)\n",
        "        else:\n",
        "          scientific_to_common_dict[scientific_name] = [common_name]\n",
        "\n",
        "  return scientific_to_common_dict"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHdzmfO2NFc5"
      },
      "source": [
        "def frequent_common_from_scientific(scientific_name, scientific_to_common_dict, common_name_occurrences_dict):\n",
        "  \"\"\"\n",
        "  inputs a scientifc name and the scientific name to common name ditionary and the occurences of common names dictionary\n",
        "  computes the most often used name\n",
        "  returns the most often used name\n",
        "  \"\"\"\n",
        "\n",
        "  if scientific_name in scientific_to_common_dict.keys():    \n",
        "    translated_names = scientific_to_common_dict[scientific_name]\n",
        "    occurrences = [common_name_occurrences_dict[x] for x in translated_names]\n",
        "    most_frequent_name = translated_names[occurrences.index(max(occurrences))]\n",
        "  else:\n",
        "    most_frequent_name = scientific_name\n",
        "\n",
        "  # exception to match noaa food web\n",
        "  if most_frequent_name == 'green alga':\n",
        "    most_frequent_name = 'green algae'\n",
        "\n",
        "  return most_frequent_name"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6s7FNCMaNzwA"
      },
      "source": [
        "def load_food_web_common_names():\n",
        "  \"\"\"\n",
        "  no inputs\n",
        "  loads the pred_prey_relationship csv file and extracts the names of the species\n",
        "  returns a set of species names\n",
        "  \"\"\"\n",
        "  food_web_common_names = set()\n",
        "  \n",
        "  with open(pred_prey_file, 'r') as pred_prey_relationships:\n",
        "    for relationship in pred_prey_relationships:      \n",
        "      common_names = relationship.split(',')\n",
        "      for common_name in common_names:\n",
        "        common_name = common_name.strip().lower()\n",
        "        if common_name != 'predators' and common_name != 'prey': # ignore title\n",
        "          food_web_common_names.add(common_name)\n",
        "\n",
        "  return food_web_common_names"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoCYXFYDN2jY"
      },
      "source": [
        "def find_related_species_names_from_common_names(food_web_common_names):\n",
        "  \"\"\"\n",
        "  inputs the food_web_common_names\n",
        "  reads throught the waterlife name file\n",
        "    creates the related_names_dict and adds names if they are present in the common names column\n",
        "  returns related_names_dict\n",
        "  \"\"\"\n",
        "\n",
        "  related_names_dict = dict()\n",
        "  df = pd.DataFrame(pd.read_excel(waterlife_data_excel_file))\n",
        "\n",
        "  for idx, row in df.iterrows():\n",
        "    for common_name in food_web_common_names:\n",
        "      if common_name[:-1] in str(row[1]):\n",
        "        species_names = row[1].strip().lower().split('/')\n",
        "        for i, species_name in enumerate(species_names):\n",
        "          species_name = species_name.strip()\n",
        "          species_name = species_name.replace('(','')\n",
        "          species_name = species_name.replace(')','')\n",
        "          scientific_name = row[0].lower().strip()\n",
        "          add_to_dict = False\n",
        "          if i == 0:\n",
        "            exact_match = False\n",
        "            false_match = False\n",
        "            if species_name == common_name:\n",
        "              exact_match = True\n",
        "            else:\n",
        "              if species_name in food_web_common_names:\n",
        "                false_match = True\n",
        "          if exact_match:\n",
        "            if i == 0:\n",
        "              add_to_dict = True\n",
        "            else:\n",
        "              if species_name not in food_web_common_names:\n",
        "                add_to_dict = True\n",
        "          else:\n",
        "            if false_match == False:\n",
        "              add_to_dict = True\n",
        "          if i < 3 and add_to_dict: # tradeoff between getting more names and accuracy\n",
        "              if common_name not in related_names_dict.keys():\n",
        "                related_names_dict[common_name] = common_name\n",
        "              if species_name not in related_names_dict.keys():\n",
        "                related_names_dict[species_name] = common_name\n",
        "              if scientific_name not in related_names_dict.keys():\n",
        "                related_names_dict[scientific_name] = common_name\n",
        "                    \n",
        "  return related_names_dict"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2L4-ItnN5Ez"
      },
      "source": [
        "def find_related_species_names_from_groupings(food_web_common_names, related_species_names, column_index):\n",
        "  \"\"\"\n",
        "  inputs the food_web_common_names, the related species names dictionary and a column index for the waterlife file\n",
        "  if the name of a food web organism could not be located by common name of an individual species,\n",
        "    the function attempts to detect based on order, class, family, or phylum and adds to the dict\n",
        "  returns the updated related_species_name_dict\n",
        "  \"\"\"\n",
        "  unmatched_names = set()\n",
        "  for common_name in food_web_common_names:\n",
        "    if common_name not in related_species_names.keys():\n",
        "      unmatched_names.add(common_name)\n",
        "\n",
        "  df = pd.DataFrame(pd.read_excel(waterlife_data_excel_file))\n",
        "  for i, row in df.iterrows():\n",
        "    for common_name in unmatched_names:\n",
        "      matching_name = common_name\n",
        "\n",
        "      # fixes inconsitency in the original data\n",
        "      if matching_name == 'blue-green algae':\n",
        "        matching_name = 'blue-green'\n",
        "      if matching_name == 'cyclopoid copepods':\n",
        "        matching_name = 'cyclopoida'\n",
        "      if matching_name == 'calanoid copepods':\n",
        "        matching_name = 'calanoida'\n",
        "      if matching_name == 'chironomids':\n",
        "        matching_name = 'chironomidae'\n",
        "      if matching_name == 'oligochaetes':\n",
        "        matching_name = 'annelida'\n",
        "      if matching_name == 'protozoans':\n",
        "        matching_name = 'flagellate'\n",
        "\n",
        "      grouping_name = str(row[column_index]).lower().strip()\n",
        "\n",
        "      scientific_name = str(row[0]).lower().strip()\n",
        "      if matching_name[:-1] in grouping_name:\n",
        "        if common_name not in related_species_names.keys():\n",
        "          related_species_names[common_name] = common_name\n",
        "        if matching_name not in related_species_names.keys():\n",
        "          related_species_names[matching_name] = common_name\n",
        "        if grouping_name not in related_species_names.keys():\n",
        "          related_species_names[grouping_name] = common_name\n",
        "        if scientific_name not in related_species_names.keys():\n",
        "          related_species_names[scientific_name] = common_name\n",
        "    \n",
        "  return related_species_names"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wrJBqMsN7RF"
      },
      "source": [
        "def load_name_exceptions(related_species_names):\n",
        "  \"\"\"\n",
        "  inputs related_species_names dictionary\n",
        "  adds hard coded exceptions due to naming inconsitencies in the original data\n",
        "  returns an updated related_species_names dictionary\n",
        "  \"\"\"\n",
        "  \n",
        "  missing_mappings = [\n",
        "                      ('cyclopoid', 'cyclopoid copepods'),\n",
        "                      ('calanoid', 'calanoid copepods'),\n",
        "                      ('zebra mussel', 'zebra/quagga mussels'),\n",
        "                      ('quagga mussel', 'zebra/quagga mussels'),\n",
        "                      ('zebra/quagga mussels', 'zebra/quagga mussels'),\n",
        "                      ('dreissena polymorpha', 'zebra/quagga mussels'),\n",
        "                      ('dreissena bugensis', 'zebra/quagga mussels'),\n",
        "                      ('dreissena rostriformis','zebra/quagga mussels'),\n",
        "                      ('bythotrephes longimanus', 'invasive waterfleas'),\n",
        "                      ('cercopagis pengoi', 'invasive waterfleas'),\n",
        "                      ('daphnia galeata galeata', 'invasive waterfleas'),\n",
        "                      ('daphnia lumholtzi', 'invasive waterfleas'),\n",
        "                      ('eubosmina coregoni', 'invasive waterfleas'),\n",
        "                      ('invasive waterfleas', 'invasive waterfleas'),\n",
        "                      ('raptorial waterfleas', 'raptorial waterfleas'),\n",
        "                      ('native waterfleas', 'native waterfleas'),\n",
        "                      ('elimia livescens', 'mollusks'),\n",
        "                      ]\n",
        "\n",
        "  for key, value in missing_mappings:\n",
        "    if key not in related_species_names.keys():\n",
        "      related_species_names[key] = value\n",
        "\n",
        "  return related_species_names"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXuaovDvN8_q"
      },
      "source": [
        "def create_related_species_names_dict():\n",
        "  \"\"\"\n",
        "  no inputs\n",
        "  creates the related species names dict from the food web matrix and the waterlife file\n",
        "  returns related_species_names\n",
        "  \"\"\"\n",
        "\n",
        "  food_web_common_names = load_food_web_common_names()\n",
        "  related_species_names = find_related_species_names_from_common_names(food_web_common_names)\n",
        "  related_species_names = load_name_exceptions(related_species_names)\n",
        "  related_species_names = find_related_species_names_from_groupings(food_web_common_names, related_species_names, 19) # Class\n",
        "  related_species_names = find_related_species_names_from_groupings(food_web_common_names, related_species_names, 20) # Family\n",
        "  related_species_names = find_related_species_names_from_groupings(food_web_common_names, related_species_names, 21) # Coarse Grouping\n",
        "  related_species_names = find_related_species_names_from_groupings(food_web_common_names, related_species_names, 28) # Fine Grouping\n",
        "  related_species_names = find_related_species_names_from_groupings(food_web_common_names, related_species_names, 30) # Order  \n",
        "\n",
        "  return related_species_names"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WyE56oxN-tG"
      },
      "source": [
        "def get_species_ids():\n",
        "  \"\"\"\n",
        "  no inputs\n",
        "  loads the noaa_impact_file and extracts unique species ids\n",
        "  returns a set of species ids\n",
        "  \"\"\"\n",
        "\n",
        "  df = pd.DataFrame(pd.read_csv(old_impact_statements_file))\n",
        "  unique_species_id = set()\n",
        "\n",
        "  for i, row in df.iterrows():\n",
        "\n",
        "    if row[8] == 1: # great lakes region only\n",
        "      unique_species_id.add(row[1])\n",
        "      \n",
        "  return unique_species_id"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZQ2nwL9OBgI"
      },
      "source": [
        "def create_species_id_dict(related_species_names_dict, scientific_to_common_dict, common_name_occurrences_dict, invasive_species_ids):\n",
        "  \"\"\"\n",
        "  inputs the related species names, scientific to common and common name occurrences dictionaries, and invasive species id set\n",
        "  loads species id files from noaa and finds the invasive species id and extracts scientific and common names\n",
        "  returns a dictionary with species id as key and common name as value\n",
        "  \"\"\"\n",
        "\n",
        "  file_list = [noaa_invasive_species_file, noaa_watchlist_species_file]\n",
        "  species_id_dict = dict()\n",
        "  related_names = set(related_species_names_dict.keys())\n",
        "\n",
        "  for file in file_list:\n",
        "    df = pd.DataFrame(pd.read_csv(file))\n",
        "    for i, row in df.iterrows():\n",
        "      try:\n",
        "        idx = int(str(row[0]).strip())\n",
        "        if idx in invasive_species_ids:\n",
        "          invasive_species = str(row[4]).strip().lower() + ' '+ str(row[5]).strip().lower()\n",
        "          if invasive_species in related_names:\n",
        "            frequent_invasive_name = related_species_names_dict[invasive_species]\n",
        "          else:\n",
        "            frequent_invasive_name = frequent_common_from_scientific(invasive_species, scientific_to_common_dict, common_name_occurrences_dict)\n",
        "          if frequent_invasive_name + 's' in related_names:\n",
        "            frequent_invasive_name += 's'\n",
        "\n",
        "          if invasive_species == frequent_invasive_name:\n",
        "            frequent_invasive_name = row[7]\n",
        "          \n",
        "          species_id_dict[idx] = frequent_invasive_name\n",
        "      except:\n",
        "        pass\n",
        "  \n",
        "  return species_id_dict"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zEgu-8NPd7G"
      },
      "source": [
        "## Creating Predator-Prey Relationships"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJEPbxpfPhST"
      },
      "source": [
        "def excel_to_pandas(file, sheet):\n",
        "  \"\"\"\n",
        "  inputs an excel file and the sheet of the excel file\n",
        "  converts the sheet to a pandas dataframe\n",
        "  returns the dataframe\n",
        "  \"\"\"\n",
        "  if sheet == None:\n",
        "    df = pd.read_excel(file, index_col=0)\n",
        "  else:\n",
        "    df = pd.read_excel(file, index_col=0, sheet_name=sheet)\n",
        "  return df.dropna(axis=0, how=\"all\")"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVgDcC8kPhY4"
      },
      "source": [
        "def get_foodweb_species(df):\n",
        "  \"\"\"\n",
        "  inputs a dataframe\n",
        "  cleans the species data in the data frame\n",
        "  reutrns a list of species from the dataframe\n",
        "  \"\"\"\n",
        "  df.index, df.columns = df.index.str.lower(), df.columns.str.lower()\n",
        "  \n",
        "  df.columns = [re.sub(\" [\\(\\[].*?[\\)\\]]\", \"\", x) for x in df.columns] \n",
        "  df.index = [re.sub(\" [\\(\\[].*?[\\)\\]]\", \"\", x) for x in df.index] \n",
        "  \n",
        "  df.columns = [re.sub(\"waterflea$\", \"waterfleas\", x) for x in df.columns]\n",
        "  df.index = [re.sub(\"waterflea$\", \"waterfleas\", x) for x in df.index]\n",
        "  df.index = [re.sub(\"waterlfea$\", \"waterfleas\", x) for x in df.index]\n",
        "  \n",
        "  df.columns = [re.sub(\"poc\", \"organic detritus\", x) for x in df.columns]\n",
        "  df.index = [re.sub(\"poc\", \"organic detritus\", x) for x in df.index]\n",
        "  \n",
        "  return df.index.tolist(), df.columns.values.tolist()"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvusRUJuPhgF"
      },
      "source": [
        "def get_waterway_pred_prey_pairs(file, sheet):\n",
        "  \"\"\"\n",
        "  inputs an excel file and a sheet from that excel file\n",
        "  finds all predetator / prey relationships in the sheet\n",
        "  returns a list of predator prey relationships\n",
        "  \"\"\"\n",
        "  species = excel_to_pandas(file, sheet).T\n",
        "  predators, preys = get_foodweb_species(species)\n",
        "  pred_prey_pairs = []\n",
        "  \n",
        "  for pred in predators:\n",
        "    for prey in preys:\n",
        "      if species.loc[pred,prey] == 'x':\n",
        "        pred_prey_pairs.append((pred, prey))\n",
        "  return pred_prey_pairs"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEdnimY6P8wR"
      },
      "source": [
        "def checkpath_save(filename, pair_list):\n",
        "  \"\"\"\n",
        "  inputs a file name as string a list of predator prey relationships\n",
        "  saves the list to a csv file\n",
        "  returns None\n",
        "  \"\"\"\n",
        "  with open(filename, 'w') as f:\n",
        "    write = csv.writer(f)\n",
        "    write.writerow(['Predators', 'Prey'])\n",
        "    write.writerows(pair_list)\n",
        "  return None"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zivCa26lP85o"
      },
      "source": [
        "def get_all_pred_prey_pairs(file, sheets):\n",
        "  \"\"\"\n",
        "  inputs a filename as a string and a list of sheets\n",
        "  calls the function to get all predator prey relationships\n",
        "  saves all predator prey relationships to a single file\n",
        "  \"\"\"\n",
        "  pred_prey_pairs, fields = [], ['Predator','Prey']\n",
        "  for sheet in sheets:\n",
        "    waterway_pairs = get_waterway_pred_prey_pairs(file, sheet)\n",
        "\n",
        "    for pair in waterway_pairs:\n",
        "      pred_prey_pairs.append(pair)\n",
        "  pred_prey_list = list(set(pred_prey_pairs))\n",
        "  display(len(pred_prey_list))\n",
        "  checkpath_save(pred_prey_file, pred_prey_list)\n",
        "\n",
        "  return pred_prey_pairs"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs1GMxVRj0SH"
      },
      "source": [
        "## Creating Scientific to Common Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n94VcfIIP9NM"
      },
      "source": [
        "def create_scientific_to_common_dictionary():\n",
        "  \"\"\"\n",
        "  inputs nothing\n",
        "  outputs scientific to foodweb name dictionary as dataframe\n",
        "  \"\"\"\n",
        "  related_species_names_dict = create_related_species_names_dict()\n",
        "  common_to_scientific_dict = create_common_to_scientific_dict(\n",
        "      waterlife_data_excel_file)\n",
        "  common_to_scientific_dict = add_impact_statement_names_to_dict(\n",
        "      common_to_scientific_dict)  \n",
        "  common_name_occurrences_dict = count_occurences(common_to_scientific_dict)\n",
        "  scientific_to_common_dict = invert_dict(common_to_scientific_dict)\n",
        "\n",
        "  df = pd.DataFrame([\n",
        "    [key,value] for key,value in scientific_to_common_dict.items()],\n",
        "    columns=[\"Name\",\"Value\"])\n",
        "  df['Value'] = df['Value'].apply(lambda x: x[0].strip())\n",
        "  \n",
        "  return df"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wee5Hq1WnOZ-"
      },
      "source": [
        "## Loading Impact References"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9M2bzFAljq0z"
      },
      "source": [
        "def get_html_marks():\n",
        "  \"\"\"\n",
        "  inputs nothing\n",
        "  returns a list of html tags\n",
        "  \"\"\"\n",
        "  return ['<br />', '&quot;', '&nbsp;', '- ', '<em>']"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39JpCyMsnTDJ"
      },
      "source": [
        "def get_impact_references(file_path):\n",
        "  \"\"\"\n",
        "  inputs a filename and path as string\n",
        "  remvoes html tags from journal abstracts\n",
        "  returns a dataframe\n",
        "  \"\"\"\n",
        "  print('Loading impact references...')\n",
        "  df = pd.read_csv(file_path)\n",
        "  for mark in get_html_marks():\n",
        "    df['abstract'] = df['abstract'].str.replace(mark, '')\n",
        "\n",
        "  return df.fillna('')"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FU9mpkUnjPw"
      },
      "source": [
        "## Loading Impact Statements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7GVSQ4Wninu"
      },
      "source": [
        "def get_impact_statements(file_path):\n",
        "  \"\"\"\n",
        "  inputs a filename and path as string\n",
        "  loads the file into a df and removes the em html tags\n",
        "  returns a dataframe\n",
        "  \"\"\"\n",
        "  df = pd.read_csv(file_path)\n",
        "  df['impact_desc'] = df['impact_desc'].str.replace('<em>', ''\n",
        "    ).str.replace('</em>', '')\n",
        "  return df"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtgI6jk-n2Db"
      },
      "source": [
        "## Combining Dataframes and Get Train-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqhF3405nTW2"
      },
      "source": [
        "def selected_columns():\n",
        "  \"\"\"\n",
        "  returns a list of selected columns for training\n",
        "  \"\"\"\n",
        "  return ['ID', 'study_type', 'study_location', 'title', 'abstract', 'refnum', \n",
        "          'ref_type', 'Year', 'journal',  'impact_type', 'cost', 'location']"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTIHg-ZAnTdj"
      },
      "source": [
        "def boolean_columns():\n",
        "  \"\"\"\n",
        "  returns a list of boolean columns\n",
        "  \"\"\"\n",
        "  return ['cost', 'location']"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4-r1yqunTkJ"
      },
      "source": [
        "def categorical_columns():\n",
        "  \"\"\"\n",
        "  returns a list of categorical columns\n",
        "  \"\"\"\n",
        "  return ['impact_type', 'ref_type'] "
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPbk0SdonTrK"
      },
      "source": [
        "def categorical_features():\n",
        "  \"\"\"\n",
        "  returns a list of categorical training columns\n",
        "  \"\"\"\n",
        "  return ['impact_type', 'ref_type']"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U97JRNxOnTx_"
      },
      "source": [
        "def text_columns():\n",
        "  \"\"\"\n",
        "  returns a list of text columns\n",
        "  \"\"\"\n",
        "  return ['title', 'abstract', 'journal']"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1xtn2rxnT5g"
      },
      "source": [
        "def label_columns():\n",
        "  \"\"\"\n",
        "  returns a list of label (y) columns\n",
        "  \"\"\"\n",
        "  return ['study_type', 'study_location']"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKMGPqi4nUBM"
      },
      "source": [
        "def make_dict(values_list):\n",
        "  \"\"\"\n",
        "  inputs a list of values\n",
        "  returns a dictionary with each value a key and an unique integer as a value\n",
        "    and a dictionary with each integer as a value and each integer as a key\n",
        "  \"\"\"\n",
        "  dictionary, counter = {}, 1\n",
        "  for value in values_list:\n",
        "    dictionary[value], dictionary[counter] = counter, value\n",
        "    counter += 1\n",
        "  dictionary[0] = 'Unknown'\n",
        "  \n",
        "  return dictionary"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-GTRFfznUN3"
      },
      "source": [
        "def merge_reference_impacts(references_df, impacts_df):\n",
        "  \"\"\"\n",
        "  inputs reference and impact dataframes\n",
        "  returns a dataframe merged with selected columns\n",
        "  \"\"\"\n",
        "  return pd.merge(references_df, impacts_df, on='refnum'\n",
        "                  )[selected_columns()]"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcbqcEcZoDCQ"
      },
      "source": [
        "def convert_to_boolean(df, columns):\n",
        "  \"\"\"\n",
        "  inputs a dataframe and target columns\n",
        "  returns the dataframe with boolean values in the selected columns\n",
        "  \"\"\"\n",
        "  for column in columns:\n",
        "    df[column] = df[column].isnull().astype('int')\n",
        "  return df"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qEcgOgmoDLE"
      },
      "source": [
        "def convert_category(df,column):\n",
        "  \"\"\"\n",
        "  inputs a dataframe and column\n",
        "  returns a dataframe with integers in selcted column and integer dictionary\n",
        "    for reverse lookup\n",
        "  \"\"\"\n",
        "  dictionary = make_dict(list(df[column].unique()))\n",
        "  df[column] = df[column].map(dictionary)\n",
        "  return df, dictionary"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJPbfxjMoGy5"
      },
      "source": [
        "def clean_text(text):\n",
        "  \"\"\"\n",
        "  inputs a string\n",
        "  returns a clean lower-cased string without punctuation\n",
        "  \"\"\"\n",
        "  text = str(text)\n",
        "  text = re.sub(r'[0-9]+', '', text)\n",
        "  text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
        "  return text.strip()"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CglFNC3oG61"
      },
      "source": [
        "def get_clean_text(df, columns):\n",
        "  \"\"\"\n",
        "  inputs a dataframe and a list of selected text columns\n",
        "  returns a dataframe with the selected columns cleaned\n",
        "  \"\"\"\n",
        "  for column in columns:\n",
        "    df[column] = df[column].apply(clean_text)\n",
        "  return df"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PKW06KjoHC-"
      },
      "source": [
        "def prepare_features(df):\n",
        "  \"\"\"\n",
        "  inputs a dataframe \n",
        "  returns the dataframe with integer values in the selected columns\n",
        "  \"\"\"\n",
        "  df = convert_to_boolean(df, boolean_columns())\n",
        "  df = get_clean_text(df, text_columns())\n",
        "  df, study_type_dictionary = convert_category(df, label_columns()[0])\n",
        "  df, study_location_dictionary = convert_category(df, label_columns()[1])\n",
        "\n",
        "  return df, study_type_dictionary, study_location_dictionary"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7yZ_ri0oHHf"
      },
      "source": [
        "def get_features(references_df, impacts_df):\n",
        "  \"\"\"\n",
        "  inputs references df and impacts df\n",
        "  calls the prepare features function\n",
        "  returns the result of the perpeare features function\n",
        "  \"\"\"\n",
        "  return prepare_features(merge_reference_impacts(references_df,impacts_df))"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKEvzx-koDTd"
      },
      "source": [
        "def check_balance(df, predict_columns):\n",
        "  \"\"\"\n",
        "  inputs a df and prediction columns\n",
        "  performs an integrity check on the balance of values \n",
        "  returns none\n",
        "  \"\"\"\n",
        "  print(\"Checking the balance of values in selected columns...\")\n",
        "  for predict_column in predict_columns:\n",
        "    display(predict_column)\n",
        "    for value in df[predict_column].unique():\n",
        "      display(\"{}: {}\".format(value, len(df[df['study_type']==value])))\n",
        "  return"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnCLJyFrofVr"
      },
      "source": [
        "## Processing New Impact Statements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUkxjIZHoj2j"
      },
      "source": [
        "def load_extracted_impact_statements(file_path):\n",
        "  \"\"\"\n",
        "  inputs a file of impact statements\n",
        "  processes the file to add study type and location\n",
        "  returns a df\n",
        "  \"\"\"\n",
        "  print('Processing new impact statements...')\n",
        "  # df = pd.read_excel(file_path).fillna('')\n",
        "  df = pd.read_csv(file_path, delimiter=\"|\").fillna('')\n",
        "  df['greatlakes_region'] = 1\n",
        "  df.rename(columns={'refnum':'reference', 'species_ID':'species'}, inplace=True)\n",
        "  df = df.drop(columns=['study_type', 'study_location'])\n",
        "  df = df[df['ID'].notna()]\n",
        "  df.ID = df.ID.astype(int)\n",
        "  \n",
        "  return df"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om_KCsGdoj_i"
      },
      "source": [
        "def load_references_match(file_path):\n",
        "  \"\"\"\n",
        "  ipnuts a file\n",
        "  drops the duplicate references from the file\n",
        "  returns a df of references and reference numbers\n",
        "  \"\"\"\n",
        "  df = pd.read_excel(file_path)\n",
        "  df= df.drop_duplicates(subset=\"reference\") \n",
        "  return df[['reference', 'refnum']]"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp-AKl9gokEx"
      },
      "source": [
        "def load_existing_references(filepath):\n",
        "  \"\"\"\n",
        "  inputs a file an excel file of journal references\n",
        "  returns a dataframe of the excel file\n",
        "  \"\"\"\n",
        "  df = pd.read_excel(filepath)\n",
        "  return df[['refnum', 'ref_type', 'author', 'Year', 'title', \n",
        "             'journal',  'abstract', 'impacts_entered']]"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFAeuqTNokNb"
      },
      "source": [
        "def match_references(unmatched_statements, reference_match, references):\n",
        "  \"\"\"\n",
        "  inputs statements and references\n",
        "  matches the statements with the reference\n",
        "  returns a merged dataframe\n",
        "  \"\"\"\n",
        "  df = pd.merge(unmatched_statements.drop(columns=['notes', 'impacted_TSNs']), \n",
        "                reference_match, how='left', on=['reference'])\n",
        "  df = df.fillna({'refnum': 'NEW'})\n",
        "  df = df[df['refnum'] != 'NEW']\n",
        "  df.refnum = df.refnum.astype(int)\n",
        "  df = df.drop(['reference'], axis=1)\n",
        "\n",
        "  return pd.merge(df, references, on=[\"refnum\"], how=\"left\").fillna('')"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLJtM8AnokWT"
      },
      "source": [
        "def prepare_prediction_features(df):\n",
        "  \"\"\"\n",
        "  inputs a dataframe \n",
        "  returns the dataframe with integer values in the selected columns\n",
        "  \"\"\"\n",
        "  df = convert_to_boolean(df, boolean_columns())\n",
        "  df = get_clean_text(df, text_columns())\n",
        "  df['study_type'], df['study_location'] = 10,10\n",
        "\n",
        "  return df[['ID', 'study_type', 'study_location', 'title', 'abstract', 'refnum',\n",
        "       'ref_type', 'Year', 'journal', 'impact_type', 'cost', 'location']]"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riYXFHUEr7tk"
      },
      "source": [
        "## Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvOBiTwqokxq"
      },
      "source": [
        "def get_train_test_split(df, label_column, test_percentage):\n",
        "  \"\"\"\n",
        "  inputs dataframe, column of labels, and test set percentage\n",
        "  outputs X_train, X_test, y_train, and y_test\n",
        "  \"\"\"\n",
        "  X, y = df.drop([label_column], axis=1), df[label_column].astype('int')\n",
        "  X_train, X_test, y_train, y_test = train_test_split(\n",
        "      X, y, test_size=test_percentage, random_state=42)\n",
        "  return X_train, X_test, y_train, y_test"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2x-UdMeFok-G"
      },
      "source": [
        "def get_column_transformer():\n",
        "  \"\"\"\n",
        "  outputs a column transformer\n",
        "  \"\"\"\n",
        "  return ColumnTransformer(transformers = [\n",
        "      ('boolean', Normalizer(norm=\"l1\"), boolean_columns()),\n",
        "      ('categorical', OneHotEncoder(handle_unknown = 'ignore'), \n",
        "                                    categorical_features()),\n",
        "      ('tfidf-1', TfidfVectorizer(max_features=5000), text_columns()[0]),\n",
        "      ('tfidf-2', TfidfVectorizer(max_features=10000), text_columns()[1]),\n",
        "        ],remainder='drop')"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfaQ-xmTolJx"
      },
      "source": [
        " def get_trained_model(df,label_column, test_percentage):\n",
        "   \"\"\"\n",
        "   inputs a dataframe, a column of labels and the percentage of the test set\n",
        "   outputs a column transformer and classifier\n",
        "   \"\"\"\n",
        "   print('Training the model...')\n",
        "   X_train, X_test, y_train, y_test = get_train_test_split(\n",
        "       df, label_column, test_percentage) \n",
        "   column_transformer = get_column_transformer()\n",
        "   X_train = column_transformer.fit_transform(X_train.fillna(''))\n",
        "   X_test = column_transformer.transform(X_test.fillna(''))\n",
        "   clf = RandomForestClassifier(n_estimators=1000, warm_start=True,\n",
        "        random_state=42).fit(X_train, y_train) \n",
        "   y_pred = clf.predict(X_test)\n",
        "   print(\"Model Accuracy: {}\".format(accuracy_score(y_test, y_pred)))\n",
        "   \n",
        "   return column_transformer, clf"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWaq88gJsfy6"
      },
      "source": [
        "## Getting Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nns50ulqolnj"
      },
      "source": [
        "def get_predictions(df, column, column_transformer, clf):\n",
        "  \"\"\"\n",
        "  inputs df, column, column transformer, and classifier\n",
        "  outputs a dataframe with labeled values\n",
        "  \"\"\"\n",
        "  print('Getting predictions...')\n",
        "  X_pred = df.copy().drop([column], axis=1)\n",
        "  X_pred = column_transformer.transform(X_pred.fillna(''))\n",
        "  y_pred = clf.predict(X_pred)\n",
        "  df[column] = y_pred\n",
        "\n",
        "  return df"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymwQxTvDstxK"
      },
      "source": [
        "## Converting for Database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBX19qsbsieB"
      },
      "source": [
        "def column_dictionary(stype_dict, slocation_dict):\n",
        "  \"\"\"\n",
        "  outputs a dictionary of dictionaries of keys and values for columns\n",
        "  \"\"\"\n",
        "  # return {'study_type': study_type_dict,\n",
        "  #         'study_location': study_location_dict, \n",
        "  return {'study_type': stype_dict,\n",
        "          'study_location': slocation_dict, \n",
        "          'species_ID': pd.read_csv(species_to_species_id_file, index_col=0, squeeze=True).to_dict()}"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Qqo7NYPsyri"
      },
      "source": [
        "def tsn_dictionary(filepath):\n",
        "  \"\"\"\n",
        "  outputs a tsn dictionary\n",
        "  \"\"\"\n",
        "  return pd.read_csv(filepath, index_col=0, squeeze=True).to_dict()"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kX2s276ssy0p"
      },
      "source": [
        "def convert_to_tsn(species_list):\n",
        "  \"\"\"\n",
        "  imports a list of species\n",
        "  exports a list of tsns\n",
        "  \"\"\"\n",
        "  tsn_dict, count, tsns = tsn_dictionary(tsn_file), 0, \"\"\n",
        "\n",
        "  species_list = species_list.replace('[', '').replace(']',''\n",
        "    ).replace(\"'\", '').split(', ')\n",
        "\n",
        "  for species in species_list:\n",
        "    if species == '': continue\n",
        "    if count == 0: \n",
        "      tsns += str(tsn_dict[species.lower()])\n",
        "    else: \n",
        "      tsns += \", \" + str(tsn_dict[species.lower()])\n",
        "    count += 1\n",
        "    \n",
        "  return tsns"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbjKfFgesy4_"
      },
      "source": [
        "def process_impact_statements(untransformed_df, predicted_df, column_dicts,\n",
        "                              impacts):\n",
        "  \"\"\"\n",
        "  imports a dataframe of untransformed impact statements, a dataframe of \n",
        "    predicted values, and a dictionary of dictionaries of numbers to values\n",
        "  outputs a dataframe with complete values\n",
        "  \"\"\"\n",
        "  print('Converting data for database...')\n",
        "  df = pd.merge(untransformed_df, predicted_df[\n",
        "    ['ID', 'study_type', 'study_location', 'refnum']], on=[\"ID\"], how=\"left\").fillna(0)\n",
        "  df.refnum = df.refnum.astype(int)\n",
        "\n",
        "  df['impacted_species'] = df['impacted_TSNs']\n",
        "  df['impacted_TSNs'] = df['impacted_TSNs'].apply(lambda x: convert_to_tsn(x))\n",
        "  df['species_ID'] = df['species']\n",
        "\n",
        "  for column in column_dicts:\n",
        "    df[column] = df[column].map(column_dicts[column])\n",
        "\n",
        "  columns = list(impacts.columns.values)\n",
        "  columns.extend(['species', 'impacted_species'])\n",
        "\n",
        "  return df[columns]"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGEPg-5qjuwr"
      },
      "source": [
        "## Creating Database Upload for NOAA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tssu-FVsP9Fc"
      },
      "source": [
        "def create_database_upload_for_NOAA():\n",
        "  \"\"\"\n",
        "  no inputs\n",
        "  runs all the functions to extract impact statements from start to finish\n",
        "    - download pdf files\n",
        "    - extract data from pdf files\n",
        "    - run machine learning models\n",
        "    - write full statements to excel file to handoff to NOAA\n",
        "  returns None\n",
        "  \"\"\"\n",
        "  download_NOAA_technical_memorandums(NOAA_url, technical_files)\n",
        "  predator_prey_pairs = get_all_pred_prey_pairs(foodweb_file, ['Lake Michigan'])\n",
        "  create_impact_line_file()\n",
        "  statement_break_classifier = build_statement_break_classifier()\n",
        "  aggregate_impact_statements(statement_break_classifier)\n",
        "  extract_impact_relationships()\n",
        "  extract_journal_references()\n",
        "  extract_impact_references()\n",
        "  create_scientific_to_common_dictionary(\n",
        "      ).to_csv(scientific_to_common_file, index=False)\n",
        "\n",
        "  impact_references = get_impact_references(impact_references_file)\n",
        "  impact_statements = get_impact_statements(old_impact_statements_file)\n",
        "  features, study_type_dict, study_location_dict = get_features(\n",
        "    impact_references, impact_statements)\n",
        "  \n",
        "  unmatched_impacts = load_extracted_impact_statements(database_upload_file)\n",
        "  unfinished_impact_statements = prepare_prediction_features(\n",
        "    match_references(unmatched_impacts, \n",
        "      load_references_match(reference_match_file), \n",
        "      load_existing_references(existing_reference_path)))\n",
        "  study_type_transformer, study_type_classifer = get_trained_model(\n",
        "      features, 'study_type', 0.1)\n",
        "  study_location_transformer, study_location_classifer = get_trained_model(\n",
        "      features, 'study_location', 0.1)\n",
        "  new_impact_statements = get_predictions(\n",
        "      unfinished_impact_statements, 'study_type', study_type_transformer, \n",
        "      study_type_classifer)\n",
        "  new_impact_statements = get_predictions(\n",
        "      unfinished_impact_statements, 'study_location', study_location_transformer, \n",
        "      study_location_classifer)\n",
        "  print('Done predicting...')\n",
        "  processed_impact_statements = process_impact_statements(\n",
        "      unmatched_impacts, new_impact_statements, \n",
        "      column_dictionary(study_type_dict, study_location_dict),\n",
        "      impact_statements)\n",
        "  processed_impact_statements.to_excel(impact_statements_file)\n",
        "\n",
        "  return None"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WeG6FmU7Khj"
      },
      "source": [
        "# Impact Statements to Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lu90k1vODS7h"
      },
      "source": [
        "## Creating tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOSeevU53cFZ"
      },
      "source": [
        "def dictionary_to_csv(dictionary, filepath):\n",
        "  \"\"\"\n",
        "  inputs a dictionary and a filepath\n",
        "  outputs (saves) a csv file \n",
        "  \"\"\"\n",
        "  with open(filepath, 'w') as file:\n",
        "    writer = csv.writer(file)\n",
        "    for key, value in dictionary.items():\n",
        "      writer.writerow([key, value])"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXzVNet2-xRz"
      },
      "source": [
        "def csv_to_dataframe(filepath):\n",
        "  \"\"\"\n",
        "  inputs a csv filepath\n",
        "  outputs a dataframe\n",
        "  \"\"\"\n",
        "  return pd.read_csv(filepath)"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2i0crsC-xe9"
      },
      "source": [
        "def csv_to_dict(filepath):\n",
        "  \"\"\"\n",
        "  inputs a csv filepath\n",
        "  outputs a dictionary\n",
        "  \"\"\"\n",
        "  return pd.read_csv(filepath, header=None, index_col=0, squeeze=True).to_dict()"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zRcGYt2BIi2"
      },
      "source": [
        "## Creating Species-Key Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOyGq1GxBOuy"
      },
      "source": [
        "def combined_relationships(invasive_filepath, foodweb_filepath):\n",
        "  \"\"\"\n",
        "  inputs a filepath of invasive impact filepath and a predator prey filepath\n",
        "  outputs a combined impacter-impacted dataframe\n",
        "  \"\"\"\n",
        "  print('Creating species-key dictionary')\n",
        "  invasive_df = pd.read_csv(invasive_filepath, delimiter='|')\n",
        "  foodweb_df = csv_to_dataframe(foodweb_filepath)\n",
        "\n",
        "  df = invasive_df.drop(columns=['tex_invasive', 'text_impacted'])\n",
        "  df.rename(columns={'translated_invasive': 'impacter', \n",
        "                     'translated_impacted':'impacted'}, inplace=True)\n",
        "  \n",
        "  foodweb_df.rename(columns={'Predators': 'impacter', \n",
        "                             'Prey':'impacted'}, inplace=True)\n",
        "  \n",
        "  return df.append(foodweb_df)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDm6zl7TBOzo"
      },
      "source": [
        "def species_index_dicts(species_pairs_df):\n",
        "  \"\"\"\n",
        "  inputs a file with value-to-value relationships\n",
        "  outputs a value to index dictionary and an index to value dictionary\n",
        "  \"\"\"\n",
        "\n",
        "  index_species_dictionary, species_index_dictionary = {}, {}\n",
        "  species = list(set(species_pairs_df.impacter) | set(species_pairs_df.impacted))\n",
        "  \n",
        "  for index in range(0, len(species)):\n",
        "    species_index_dictionary[species[index]] = index\n",
        "    index_species_dictionary[index] = species[index]\n",
        "\n",
        "  return species_index_dictionary, index_species_dictionary"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhltLdm_Daka"
      },
      "source": [
        "## Getting Impacter-Impacted Distance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFrwnENc-xmG"
      },
      "source": [
        "def combined_relationships(invasive_filepath, foodweb_filepath):\n",
        "  \"\"\"\n",
        "  inputs a filepath of invasive impact filepath and a predator prey filepath\n",
        "  outputs a combined impacter-impacted dataframe\n",
        "  \"\"\"\n",
        "  print('Getting impacter-impacted distance...')\n",
        "  invasive_df = pd.read_csv(invasive_filepath, delimiter='|')\n",
        "  foodweb_df = csv_to_dataframe(foodweb_filepath)\n",
        "\n",
        "  df = invasive_df.drop(columns=['tex_invasive', 'text_impacted'])\n",
        "  df.rename(columns={'translated_invasive': 'impacter', \n",
        "                     'translated_impacted':'impacted'}, inplace=True)\n",
        "  \n",
        "  foodweb_df.rename(columns={'Predators': 'impacter', \n",
        "                             'Prey':'impacted'}, inplace=True)\n",
        "  \n",
        "  return df.append(foodweb_df)"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJHJqFiJ-xy7"
      },
      "source": [
        "def add_distance_between_nodes(species_pairs_df):\n",
        "  \"\"\"\n",
        "  inputs a dataframe of value pairs\n",
        "  ouputs a dataframe of value pairs with the shortest distance therebetween\n",
        "  \"\"\"\n",
        "  species_pairs_distance, relationships = [], species_pairs_df.values.tolist()\n",
        "  G = nx.DiGraph()\n",
        "  G.add_edges_from(relationships)\n",
        "  shortest_path = dict(nx.all_pairs_shortest_path_length(G))\n",
        "\n",
        "  species = list(set(species_pairs_df.impacter) | set(species_pairs_df.impacted))\n",
        "\n",
        "  for species1 in species:\n",
        "    for species2 in species:\n",
        "      if species1 == species2: continue\n",
        "      if nx.has_path(G, species1, species2):\n",
        "        species_pairs_distance.append(\n",
        "            [species1, species2, shortest_path[species1][species2]])\n",
        "\n",
        "  return pd.DataFrame(species_pairs_distance, \n",
        "                      columns=['impacter', 'impacted', 'distance'])"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mcoI1BF-x-4"
      },
      "source": [
        "def key_relationships(species_species_df, species_index_dictionary):\n",
        "  \"\"\"\n",
        "  inputs a named impacter-impacted pair dataframe and species-to-key dictionary\n",
        "  outputs a keyed impacter-impacted pair dataframe\n",
        "  \"\"\"\n",
        "  species_species_df['impacter'] = species_species_df['impacter'\n",
        "      ].map(species_index_dictionary)\n",
        "  species_species_df['impacted'] = species_species_df['impacted'\n",
        "      ].map(species_index_dictionary)\n",
        "\n",
        "  return species_species_df"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7lLYDg_QktT"
      },
      "source": [
        "## Getting Waterways and Cooordinates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JtGFWnRQoKa"
      },
      "source": [
        "def json_to_df(file_name):\n",
        "  \"\"\"\n",
        "  inputs a file containing geojson data\n",
        "  converts the dict object to a dataframe\n",
        "    removes rows with no waterway name\n",
        "    creates a row for each coordinate in the data frame (from the list of \n",
        "    coordinates)\n",
        "  returns the expanded dataframe\n",
        "  \"\"\"\n",
        "  print('Getting waterways and coordinates...')\n",
        "  # clean data frame\n",
        "  data = json.load(open(file_name))\n",
        "  df = pd.json_normalize(data[\"features\"])\n",
        "  df = df.rename(columns={'properties.GNIS_NAME': 'properties.name'}) #rename USGS data vs. purchased data\n",
        "  df.columns= df.columns.str.lower()\n",
        "  df = df[df['properties.name'].notna()]\n",
        "  df = df[['properties.name','geometry.type','geometry.coordinates']]    \n",
        "\n",
        "  # flatten data frame based on geometry (line or polygon)\n",
        "  coordinate_list = []\n",
        "  for index, row in df.iterrows():\n",
        "    water_body_name = row[0]\n",
        "    if row[1] == 'LineString':\n",
        "      for coordinates in row[2]:\n",
        "        coordinate_list.append([water_body_name, coordinates[1], coordinates[0]])\n",
        "    elif row[1] == 'Polygon':\n",
        "      for shape in row[2]:\n",
        "        for coordinates in shape:\n",
        "          coordinate_list.append([water_body_name, coordinates[1], coordinates[0]])\n",
        "    else:\n",
        "      print('Error {} is an unkown geometry type'.format(row[1]))\n",
        "\n",
        "  df = pd.DataFrame(coordinate_list, columns=['name','latitude', 'longitude'])\n",
        "\n",
        "  print('{} coordinates loaded from {}.'.format(len(df), file_name))\n",
        "  \n",
        "  return df"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjpGPC2KQoXh"
      },
      "source": [
        "def create_water_bodies_df(file_name_list):\n",
        "  \"\"\"\n",
        "  inputs a list file_names\n",
        "  creates data frames from each of the water source files and concatenates them \n",
        "  returns a data frame with name and coordinates of all water_bodies\n",
        "  \"\"\"\n",
        "\n",
        "  water_bodies_df = pd.DataFrame()\n",
        "\n",
        "  for file_name in file_name_list:\n",
        "    \n",
        "    water_body_df = json_to_df(file_name)\n",
        "    water_bodies_df = pd.concat([water_bodies_df, water_body_df]\n",
        "      ).reset_index(drop=True)\n",
        "\n",
        "  return water_bodies_df"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0aHY7IbRLl0"
      },
      "source": [
        "## Getting Invasion List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1I_Cza6Qosv"
      },
      "source": [
        "def simplified_invasions(invasions_file, scientific_to_name_filepath):\n",
        "  \"\"\"\n",
        "  inputs path to invasive species specimen file\n",
        "  returns simplified invsasive species specimen dateframe filtered by\n",
        "    monitored species\n",
        "  \"\"\"\n",
        "  print('Getting invasion list...')\n",
        "  scientific_to_name_dict = pd.read_csv(scientific_to_name_filepath)\n",
        "  scientific_to_name_dict = dict(zip(\n",
        "      scientific_to_name_dict.Name, scientific_to_name_dict.Value))\n",
        "  scientific_names = list(scientific_to_name_dict.keys())\n",
        "  invasions_df = pd.read_csv(invasions_file, low_memory=False)\n",
        "  invasions_df['Date'] = pd.to_datetime(\n",
        "      invasions_df[['Year', 'Month', 'Day']]).dt.date\n",
        "  invasions_df['Scientific Name'] = invasions_df['Scientific Name'\n",
        "      ].apply(lambda x: x.lower().strip())\n",
        "  invasions_df = invasions_df[invasions_df['Scientific Name'\n",
        "      ].isin(scientific_names)]\n",
        "\n",
        "  return invasions_df[[\n",
        "      'Specimen Number', 'Species ID', 'Scientific Name', 'Common Name', \n",
        "      'Latitude', 'Longitude', 'Date']].reindex()"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSkpxNkORSvG"
      },
      "source": [
        "## Locating Closest Waterway Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVYmZiiURRWK"
      },
      "source": [
        "def get_waterway_pickle(waterways_df):\n",
        "  \"\"\"\n",
        "  inputs waterways dataframe\n",
        "  outputs a pickle file of the waterways\n",
        "  \"\"\"\n",
        "  print('Locating closest waterway features...')\n",
        "  kd = cKDTree(waterways_df[['latitude',\t'longitude']].values) \n",
        "  pickle.dump(kd,open(waterways_file,'wb'))\n",
        "\n",
        "  return kd"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hb1Ax6zRRcr"
      },
      "source": [
        "def get_closest_waterway(observed_df, waterways_df, waterways_tree):\n",
        "  \"\"\"\n",
        "  inputs observed specimen dateframe, waterways dataframe, and waterways pickle\n",
        "  finds the index of the closest waterways and maps the name to closest waterway\n",
        "  outputs observed_df with closest waterway\n",
        "  \"\"\"\n",
        "\n",
        "  observed_df = observed_df.dropna().reset_index().drop(['index'],  axis=1)\n",
        "  \n",
        "  distances, indices = waterways_tree.query(\n",
        "      observed_df[[\"Latitude\", \"Longitude\"]], k = 1)\n",
        "\n",
        "  observed_df['closest waterway'] = pd.Series(indices).map(waterways_df['name']) \n",
        "\n",
        "  return observed_df"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyBYCYVoRb5j"
      },
      "source": [
        "## Calculating Waterway Distance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsokbPhyRRpb"
      },
      "source": [
        "def observation_locations(filepath):    \n",
        "  \"\"\"\n",
        "  inputs a filepath\n",
        "  outputs a dateframe with species ID and closest waterway\n",
        "  \"\"\"     \n",
        "  return pd.read_csv(filepath)[['Species ID', 'closest waterway']\n",
        "                               ].drop_duplicates()"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Az7cLp82RR1q"
      },
      "source": [
        "def waterway_edges(filepath):\n",
        "  \"\"\"\n",
        "  inputs a filepath\n",
        "  outputs a dateframe with to and from columns\n",
        "  \"\"\"\n",
        "  return pd.read_csv(filepath).drop(['Unnamed: 0'], axis=1)"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO3FNjWFRjn0"
      },
      "source": [
        "def get_species_water_distance(specimens_waterways_df, waterway_edges_df):\n",
        "  \"\"\"\n",
        "  inputs dataframes of specimens-waterways and waterway edges\n",
        "  outputs a dataframe with 'Species ID', 'waterway', and 'distance'\n",
        "  \"\"\"\n",
        "  print('Calculating waterway distance...')\n",
        "  presence = []\n",
        "  observations = observation_locations(specimens_locations_file)\n",
        "  flow_edges = waterway_edges(waterways_edges_file)\n",
        "  waterways = list(set(flow_edges['from']) | set(flow_edges['to']))\n",
        "  species_list = list(set(observations['Species ID']))\n",
        "\n",
        "  G = nx.Graph()\n",
        "  G.add_edges_from(flow_edges.values.tolist())\n",
        "  spl = dict(nx.all_pairs_shortest_path_length(G))\n",
        "\n",
        "  for species in species_list:\n",
        "    df = observations[observations['Species ID'] == species]\n",
        "    for waterway1 in df['closest waterway'].unique():\n",
        "      if waterway1 not in waterways: continue\n",
        "      for waterway2 in waterways:\n",
        "        if waterway1 == waterway2: continue\n",
        "        if nx.has_path(G, waterway1, waterway2):\n",
        "          presence.append([species, waterway2, spl[waterway1][waterway2]])\n",
        "  return  pd.DataFrame(presence, columns=['Species ID', 'waterway', 'distance'])"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBsl8ow6Rj8J"
      },
      "source": [
        "def species_id_to_foodweb_name(scientific_to_name_filepath, \n",
        "                               species_id_filepath):\n",
        "  \"\"\"\n",
        "  inputs filepaths for a scientific name and a species id dictionaries\n",
        "  outputs a species id to common (foodpath) name\n",
        "  \"\"\"\n",
        "  scientific_to_name_dict = pd.read_csv(scientific_to_name_filepath)\n",
        "  scientific_to_name_dict = dict(zip(\n",
        "      scientific_to_name_dict.Name, scientific_to_name_dict.Value))\n",
        "  df = pd.read_csv(species_id_filepath).drop(columns='Unnamed: 0')\n",
        "  df['species'] = df['species'].apply(lambda x: x.lower().strip())\n",
        "  df['name'] = df['species'].map(scientific_to_name_dict)\n",
        "  df = df.drop(columns='species')\n",
        "\n",
        "  return dict(zip(df['Species_ID'], df['name']))"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2inAyIVaRrnT"
      },
      "source": [
        "def foodweb_name_to_index(foodweb_to_index_filepath):\n",
        "  \"\"\"\n",
        "  inputs the food_web file\n",
        "  returns a dictionary of the food web\n",
        "  \"\"\"\n",
        "  return csv_to_dict(foodweb_to_index_filepath)"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDL1geK0blzO"
      },
      "source": [
        "def add_species_index(species_water_df, speciesid_name_dict, name_index_dict):\n",
        "  \"\"\"\n",
        "  inputs the observations of invasive species in michigan waterways\n",
        "  maps species names to related data\n",
        "  returns a data frame with the observation and their related data\n",
        "  \"\"\"\n",
        "  name_index_dict['zebra mussel'] = 28\n",
        "  species_water_df['name'] = species_water_df['Species ID'].map(speciesid_name_dict)\n",
        "  species_water_df = species_water_df[species_water_df['name'].notna()].reset_index()\n",
        "  species_water_df['key'] = species_water_df['name'].map(name_index_dict).fillna(10000).astype(int)\n",
        "\n",
        "  return species_water_df[['key', 'waterway', 'distance', 'name', 'Species ID']]"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3ukKZu0yIp1"
      },
      "source": [
        "def write_network_files():\n",
        "  \"\"\"\n",
        "  inputs none\n",
        "  writes all the network data files to tables for database uploads\n",
        "    keeping a consistency of speices IDs throughout tables\n",
        "  returns none\n",
        "  \"\"\"\n",
        "\n",
        "  print('writing network files...')\n",
        "\n",
        "  # get all species and assign to list\n",
        "  invasives = list(pd.read_csv(impact_relationships_file, delimiter='|')['translated_invasive'].unique())\n",
        "  invasives = [x.strip().lower() for x in invasives]\n",
        "  invasives = set(invasives)\n",
        "  relationships_file = impacter_impacted_distance_named_file\n",
        "  relationships_df = pd.read_csv(relationships_file)\n",
        "  relationships_df['impacter'] = relationships_df['impacter'].str.lower()\n",
        "  relationships_df['impacter'] = relationships_df['impacter'].str.strip()\n",
        "  relationships_df['impacted'] = relationships_df['impacted'].str.lower()\n",
        "  relationships_df['impacted'] = relationships_df['impacted'].str.strip()\n",
        "  relationships_df = relationships_df.drop_duplicates()\n",
        "  relationships_df = relationships_df[['impacter', 'impacted', 'distance']]\n",
        "  obs_df = pd.read_csv(observations_waterways_distances_file)\n",
        "  obs_df = obs_df.replace('zebra mussel', 'zebra/quagga mussels')\n",
        "  obs_df = obs_df.replace('cyclopoid copepod', 'cyclopoid copepods')\n",
        "  obs_df = obs_df.replace('chinook salmon', 'chinook')\n",
        "  obs_df = obs_df.replace('spiny waterflea', 'invasive waterfleas')\n",
        "  obs_df = obs_df.replace('carp', 'bighead carp') # this is an assumption as the other types of invsaive carp are already listed\n",
        "  species=invasives.copy()\n",
        "  species.update(set(relationships_df['impacter'].unique()))\n",
        "  species.update(set(relationships_df['impacted'].unique()))\n",
        "  not_in_network_list = []\n",
        "  for name in obs_df.name.unique():\n",
        "    if name not in species:\n",
        "      not_in_network_list.append(name)\n",
        "  species.update(set(not_in_network_list))\n",
        "\n",
        "  # write species table, write invasives table\n",
        "  species_id_list = []\n",
        "  invasives_id_list = []\n",
        "  species_loop = list(sorted(species))\n",
        "  for idx, specie in enumerate(species_loop):\n",
        "    species_id_list.append([idx, specie])\n",
        "    if specie in invasives:\n",
        "      invasives_id_list.append([idx, specie])\n",
        "  pd.DataFrame(species_id_list,columns=['id', 'name']).to_csv(species_file, index=False)\n",
        "  pd.DataFrame(invasives_id_list,columns=['id', 'name']).to_csv(invasive_species_file, index=False)\n",
        "  species_id_dict = {x[1] : x[0] for x in species_id_list}\n",
        "\n",
        "  # write impacter impacted relation file \n",
        "  impacter_impacted_list= []\n",
        "  for idx,row in relationships_df.iterrows():\n",
        "    if row[2]==1:\n",
        "      impacter_impacted_list.append([species_id_dict[row[0]], species_id_dict[row[1]]])\n",
        "  impacter_impacted_df = pd.DataFrame(impacter_impacted_list, columns=['impacter_id', 'impacted_id'])\n",
        "  impacter_impacted_df.to_csv(impact_rel_file, index=False)\n",
        "\n",
        "  # write species observed file\n",
        "  obs_list = []\n",
        "  for idx, row in obs_df.iterrows():\n",
        "    on_network = 0 if row[3] in not_in_network_list else 1\n",
        "    obs_list.append([idx, species_id_dict[row[3]], row[1], row[2], on_network])\n",
        "  pd.DataFrame(obs_list,columns=['uid', 'species_id', 'waterbody_name', 'distance', 'on_network']).to_csv(species_observed_file, index=False)\n",
        "\n",
        "  return None"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aPszxWVJoZ6"
      },
      "source": [
        "## Create Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyB-sp32-yNA"
      },
      "source": [
        "def create_networks():\n",
        "  \"\"\"\n",
        "  no inputs\n",
        "  runs the network creation from start to finish\n",
        "    - creates the food web networks\n",
        "    - creates the waterways networks\n",
        "    - writes all the data to files for database upload\n",
        "  returns none\n",
        "  \"\"\"\n",
        "  impacter_impacted_df = combined_relationships(\n",
        "    impact_relationships_file, pred_prey_file)\n",
        "  species_to_index, index_to_species = species_index_dicts(impacter_impacted_df)\n",
        "  dictionary_to_csv(species_to_index, species_to_index_file)\n",
        "  dictionary_to_csv(index_to_species, index_to_species_file)\n",
        "\n",
        "  relationships_named = combined_relationships(\n",
        "    impact_relationships_file, pred_prey_file)\n",
        "  relationships_named.to_csv(relationships_named_file, index=False)\n",
        "\n",
        "  species_species_distance = add_distance_between_nodes(relationships_named)\n",
        "  species_species_distance.to_csv(impacter_impacted_distance_named_file, \n",
        "                                  index=False)\n",
        "  relationships_keyed = key_relationships(\n",
        "    relationships_named, csv_to_dict(species_to_index_file))\n",
        "  relationships_keyed.to_csv(relationships_keyed_file, index=False)\n",
        "\n",
        "  water_bodies_df = create_water_bodies_df([rivers_file, lakes_file])\n",
        "  water_bodies_df.to_csv(waterways_dataframe_file, index=False)\n",
        "\n",
        "  invasion_df = simplified_invasions(invasion_file, scientific_to_common_file)\n",
        "\n",
        "  waterways = get_waterway_pickle(water_bodies_df)\n",
        "  closest_waterways = get_closest_waterway(\n",
        "      invasion_df, water_bodies_df, waterways)\n",
        "  closest_waterways.to_csv(specimens_locations_file, index=False)\n",
        "\n",
        "  species_id_to_name_dict = species_id_to_foodweb_name(\n",
        "      scientific_to_common_file, species_id_to_scientific_file)\n",
        "  \n",
        "  foodweb_name_to_index_dict = foodweb_name_to_index(species_to_index_file)\n",
        "  \n",
        "  species_water_distance = get_species_water_distance(\n",
        "      specimens_locations_file, waterways_edges_file)\n",
        "  \n",
        "  observations_plot = add_species_index(species_water_distance, \n",
        "      species_id_to_name_dict, foodweb_name_to_index_dict)\n",
        "  observations_plot.to_csv(observations_waterways_distances_file, index=False)\n",
        "  print('Networks created.')\n",
        "  write_network_files()\n",
        "\n",
        "  return"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TipQLoR7Qgd"
      },
      "source": [
        "# Networks to Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eATR_0qFBmcC"
      },
      "source": [
        "def create_tables_for_ripple_plot():\n",
        "  \"\"\"\n",
        "  inputs None\n",
        "  opens the impacter impacted named distance file\n",
        "  creates lists of impacters and impacteds and writes the csv files for db insertion\n",
        "  returns None\n",
        "  \"\"\"\n",
        "\n",
        "  print('writing tables for ripple plot')\n",
        "\n",
        "  #load and clean data\n",
        "  invasives = list(pd.read_csv(impact_relationships_file, delimiter='|')['translated_invasive'].unique())\n",
        "  relationships_file = impacter_impacted_distance_named_file\n",
        "  relationships_df = pd.read_csv(relationships_file)\n",
        "  relationships_df['impacter'] = relationships_df['impacter'].str.lower()\n",
        "  relationships_df['impacter'] = relationships_df['impacter'].str.strip()\n",
        "  relationships_df['impacted'] = relationships_df['impacted'].str.lower()\n",
        "  relationships_df['impacted'] = relationships_df['impacted'].str.strip()\n",
        "  relationships_df = relationships_df.drop_duplicates()\n",
        "  relationships_df = relationships_df[['impacter', 'impacted', 'distance']]\n",
        "\n",
        "  #create a list of impacters\n",
        "  impacters = list(relationships_df['impacter'].unique())\n",
        "  impacters = list(set(invasives).intersection(set(impacters)))\n",
        "  impacters = sorted(impacters)\n",
        "  impacters_dict = [{'label': impacter, 'value': impacter} for impacter in impacters]\n",
        "  impacters_2 = list(relationships_df['impacter'].unique())\n",
        "\n",
        "  #compute polar coordinate theta for impacters\n",
        "  relationships_list = []\n",
        "  for impacter in impacters_2:\n",
        "    relationships_list.append([impacter, impacter, 0, 0])\n",
        "    for distance in range(1,4):\n",
        "      sub_df = relationships_df[(relationships_df['impacter']==impacter) & (relationships_df['distance']==distance)].reset_index(drop=True)\n",
        "      thetas = np.linspace(0, 360, len(sub_df) + 1)\n",
        "      for idx, row in sub_df.iterrows():\n",
        "        invasive_impacter = 1 if impacter in impacters else 0\n",
        "        relationships_list.append([impacter, row[1], distance, thetas[idx]])\n",
        "  impacters_df = pd.DataFrame(relationships_list, columns=['impacter', 'impacted', 'radius', 'theta'])\n",
        "\n",
        "  #create a list of impacteds\n",
        "  impacteds = list(relationships_df['impacted'].unique())\n",
        "  impacteds = sorted(impacteds)\n",
        "  impacted_dict = [{'label': impacted, 'value': impacted} for impacted in impacteds]\n",
        "\n",
        "  #compute polar coordinate theta for impacteds\n",
        "  relationships_list = []\n",
        "  for impacted in impacteds:\n",
        "    relationships_list.append([impacted, impacted, 0, 0])\n",
        "    for distance in range(1,4):\n",
        "      sub_df = relationships_df[(relationships_df['impacted']==impacted) & (relationships_df['distance']==distance)].reset_index(drop=True)\n",
        "      sub_df = sub_df[sub_df['impacter'].isin(impacters)].reset_index(drop=True)\n",
        "      thetas = np.linspace(0, 360, len(sub_df) + 1)\n",
        "      for idx, row in sub_df.iterrows():\n",
        "        relationships_list.append([row[0], impacted, distance, thetas[idx]])\n",
        "  impacteds_df = pd.DataFrame(relationships_list, columns=['impacter', 'impacted', 'radius', 'theta2'])\n",
        "  foodweb_df = pd.merge(impacters_df, impacteds_df,  how='outer', on=['impacter','impacted','radius']).fillna(0)\n",
        "\n",
        "  # write database table for impacter_impacted distance\n",
        "  relationships_list = []\n",
        "  for idx, row in foodweb_df.iterrows():\n",
        "    invasive_impacter = 0\n",
        "    if row[0] in impacters or row[0] == row[1]:\n",
        "      invasive_impacter = 1\n",
        "    relationships_list.append([idx, row[0], row[1], row[2], int(row[3]), int(row[4]), invasive_impacter])\n",
        "  foodweb_df = pd.DataFrame(relationships_list, columns=['uid', 'impacter', 'impacted', 'radius', 'theta', 'theta_two', 'invasive_impacter'])\n",
        "  foodweb_df.to_csv(impacter_impacted_distance_file, index=False)\n",
        "\n",
        "  # write database table for impacter dropdown\n",
        "  impacter_list = []\n",
        "  for idx, impacter in enumerate(impacters):\n",
        "    impacter_list.append([idx, impacter])\n",
        "  pd.DataFrame(impacter_list,columns=['uid', 'impacter']).to_csv(invasive_impacters_dropdown_file, index=False)\n",
        "\n",
        "  # write database table for impacteds dropdown\n",
        "  impacted_list = []\n",
        "  for idx, impacted in enumerate(impacteds):\n",
        "    impacted_list.append([idx, impacted])\n",
        "  pd.DataFrame(impacted_list,columns=['uid', 'impacted']).to_csv(impacted_species_dropdown_file, index=False)\n",
        "\n",
        "  print('all scripts completed successfully - done')\n",
        "  return None"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rRajesqDsTf"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLS8mGdiBmn2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "79db2089-89f6-4a44-e283-e5086adf08b7"
      },
      "source": [
        "create_database_upload_for_NOAA()\n",
        "create_networks()\n",
        "create_tables_for_ripple_plot()\n"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading NOAA technical memorandum files....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "247"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting impact lines from tm-161.pdf\n",
            "Warning: Missing Section for Alnus glutinosa\n",
            "[182, 182, 181, 181]\n",
            "[[0, 0, 0, 0, 0, 0, 0], [182, 182, 182, 182, 182, 182, 182], [181, 181, 181, 181, 181, 181, 181], [181, 181, 181, 181, 181, 181, 181]]\n",
            "Extracting impact lines from tm-161b.pdf\n",
            "[6, 6, 6, 6]\n",
            "[[0, 0, 0, 0, 0, 0, 0], [6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6]]\n",
            "Extracting impact lines from tm-161c.pdf\n",
            "Warning: Missing Section for Cercopagis pengoi\n",
            "Warning: Missing Section for Ctenopharyngodon idella\n",
            "Warning: Missing Section for Hemimysis anomala\n",
            "Warning: Missing Section for Lepomis humilis\n",
            "Warning: Missing Section for Lepomis humilis\n",
            "Warning: Missing Section for Oncorhynchus tshawytscha\n",
            "Warning: Missing Section for Schizopera borutzkyi\n",
            "[8, 6, 2, 2]\n",
            "[[0, 0, 0, 0, 0, 0, 0], [6, 6, 6, 6, 6, 6, 6], [2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2]]\n",
            "Extracting impact lines from tm-169.pdf\n",
            "[67, 67, 67, 67]\n",
            "[[0, 0, 0, 0, 0, 0, 0], [67, 67, 67, 67, 67, 67, 67], [67, 67, 67, 67, 67, 67, 67], [67, 67, 67, 67, 67, 67, 67]]\n",
            "Extracting impact lines from tm-169b.pdf\n",
            "Warning: Missing Section for Arundo donax\n",
            "Warning: Missing Section for Calanipeda aquaedulcis\n",
            "Warning: Missing Section for Carassius carassius\n",
            "Warning: Missing Section for Channa argus\n",
            "Warning: Missing Section for Chelicorophium curvispinum\n",
            "Warning: Missing Section for Cornigerius maeoticus maeoticus\n",
            "Warning: Missing Section for Cottus gobio\n",
            "Warning: Missing Section for Crassula helmsii\n",
            "Warning: Missing Section for Daphnia cristata\n",
            "Warning: Missing Section for Ectinosoma abrau\n",
            "Warning: Missing Section for Eichhornia crassipes\n",
            "Warning: Missing Section for Pseudorasbora parva\n",
            "Warning: Missing Section for Sinelobus stanfordi\n",
            "[28, 20, 11, 9]\n",
            "[[0, 0, 0, 0, 0, 0, 0], [20, 20, 20, 20, 20, 20, 20], [11, 11, 11, 11, 11, 11, 11], [9, 9, 9, 9, 9, 9, 9]]\n",
            "Extracting impact lines from tm-169c.pdf\n",
            "Warning: Missing Section for Filinia cornuta\n",
            "Warning: Missing Section for Rutilus rutilus\n",
            "[10, 8, 8, 10]\n",
            "[[0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 8, 8, 8, 8], [8, 8, 8, 8, 8, 8, 8], [10, 10, 10, 10, 10, 10, 10]]\n",
            "Impact line file created\n",
            "Training statement break classifier\n",
            "3099 impact statements extracted\n",
            "Impact statements file created\n",
            "Extracting impact relationships between species...\n",
            "701 relationships extracted from impact statements\n",
            "Extracting references...\n",
            "465 references extracted from tm-161.pdf\n",
            "87 references extracted from tm-161b.pdf\n",
            "225 references extracted from tm-161c.pdf\n",
            "1608 references extracted from tm-169.pdf\n",
            "298 references extracted from tm-169b.pdf\n",
            "133 references extracted from tm-169c.pdf\n",
            "Extracting references...\n",
            "2492 impact statements matched to references.\n",
            "Loading impact references...\n",
            "Processing new impact statements...\n",
            "Training the model...\n",
            "Model Accuracy: 0.73\n",
            "Training the model...\n",
            "Model Accuracy: 0.79\n",
            "Getting predictions...\n",
            "Getting predictions...\n",
            "Done predicting...\n",
            "Converting data for database...\n",
            "Getting impacter-impacted distance...\n",
            "Getting impacter-impacted distance...\n",
            "Getting waterways and coordinates...\n",
            "1356349 coordinates loaded from /content/assets/Shared drives/ermiasb-rjbowman-tobyk/data/rivers.geojson.\n",
            "Getting waterways and coordinates...\n",
            "689052 coordinates loaded from /content/assets/Shared drives/ermiasb-rjbowman-tobyk/data/lake.geojson.\n",
            "Getting invasion list...\n",
            "Locating closest waterway features...\n",
            "Calculating waterway distance...\n",
            "Networks created.\n",
            "writing network files...\n",
            "writing tables for ripple plot\n",
            "all scripts completed successfully - done\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}